{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßπ DATA CLEANING AND MERGING PIPELINE\n",
    "## –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –∫—Ä–µ–¥–∏—Ç–Ω—ã—Ö –¥–µ—Ñ–æ–ª—Ç–æ–≤\n",
    "\n",
    "---\n",
    "\n",
    "### üìã –ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –Ω–æ—É—Ç–±—É–∫:\n",
    "1. ‚úÖ –ó–∞–≥—Ä—É–∂–∞–µ—Ç **6 –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤** —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ (CSV, Parquet, Excel, XML, JSONL)\n",
    "2. ‚úÖ –û—á–∏—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ (—É–¥–∞–ª—è–µ—Ç —à—É–º, –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ)\n",
    "3. ‚úÖ –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –≤ –µ–¥–∏–Ω—ã–π —Ñ–∞–π–ª\n",
    "4. ‚úÖ –°–æ–∑–¥–∞–µ—Ç **–±–∞–∑–æ–≤—ã–µ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏** (—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã, —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã)\n",
    "5. ‚úÖ –†–∞–∑–¥–µ–ª—è–µ—Ç –Ω–∞ train/test (80/20, stratified)\n",
    "6. ‚úÖ –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã\n",
    "\n",
    "### üìÇ –ò—Å—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã (–∏–∑ –ø–∞–ø–∫–∏ `drive-download-20251115T045945Z-1-001`):\n",
    "1. `application_metadata.csv` (90,000 —Å—Ç—Ä–æ–∫) - –ó–∞—è–≤–∫–∏ –Ω–∞ –∫—Ä–µ–¥–∏—Ç + —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è `default`\n",
    "2. `demographics.csv` (90,000 —Å—Ç—Ä–æ–∫) - –î–µ–º–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ\n",
    "3. `credit_history.parquet` (~90,000 —Å—Ç—Ä–æ–∫) - –ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è\n",
    "4. `financial_ratios.jsonl` (89,999 —Å—Ç—Ä–æ–∫) - –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏\n",
    "5. `loan_details.xlsx` (Excel) - –î–µ—Ç–∞–ª–∏ –∫—Ä–µ–¥–∏—Ç–∞\n",
    "6. `geographic_data.xml` (89,999 –∑–∞–ø–∏—Å–µ–π) - –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ\n",
    "\n",
    "### üì¶ –í—ã—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã:\n",
    "- `X_train_optimized.parquet` (71,999 √ó ~90 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)\n",
    "- `X_test_optimized.parquet` (18,000 √ó ~90 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)\n",
    "- `y_train.parquet`\n",
    "- `y_test.parquet`\n",
    "\n",
    "### ‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: ~3-5 –º–∏–Ω—É—Ç\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q pandas numpy scikit-learn pyarrow openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ –ò–º–ø–æ—Ä—Ç—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA CLEANING AND MERGING PIPELINE\")\n",
    "print(\"Credit Default Prediction Dataset\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤\n",
    "\n",
    "**–í–ê–ñ–ù–û:** –ó–∞–≥—Ä—É–∑–∏—Ç–µ –≤—Å—é –ø–∞–ø–∫—É `drive-download-20251115T045945Z-1-001` —Å 6 —Ñ–∞–π–ª–∞–º–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "print(\"üì§ –û–ø—Ü–∏—è 1: –ó–∞–≥—Ä—É–∑–∏—Ç–µ ZIP –∞—Ä—Ö–∏–≤ —Å –¥–∞–Ω–Ω—ã–º–∏\")\n",
    "print(\"   (–°–æ–∑–¥–∞–π—Ç–µ ZIP –∏–∑ –ø–∞–ø–∫–∏ drive-download-20251115T045945Z-1-001)\")\n",
    "print()\n",
    "print(\"üì§ –û–ø—Ü–∏—è 2: –ó–∞–≥—Ä—É–∑–∏—Ç–µ 6 —Ñ–∞–π–ª–æ–≤ –≤—Ä—É—á–Ω—É—é\")\n",
    "print()\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# If ZIP is uploaded, extract it\n",
    "for filename in uploaded.keys():\n",
    "    if filename.endswith('.zip'):\n",
    "        print(f\"\\nüì¶ –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ {filename}...\")\n",
    "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall('.')\n",
    "        print(\"‚úÖ –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n",
    "\n",
    "# Find data directory\n",
    "if os.path.exists('drive-download-20251115T045945Z-1-001'):\n",
    "    DATA_DIR = 'drive-download-20251115T045945Z-1-001'\n",
    "else:\n",
    "    DATA_DIR = '.'\n",
    "\n",
    "print(f\"\\nüìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏: {DATA_DIR}\")\n",
    "print(f\"\\nüìã –§–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏:\")\n",
    "for file in os.listdir(DATA_DIR):\n",
    "    if not file.startswith('.'):\n",
    "        size_mb = os.path.getsize(os.path.join(DATA_DIR, file)) / (1024 * 1024)\n",
    "        print(f\"   - {file} ({size_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 –ó–∞–≥—Ä—É–∑–∫–∞ application_metadata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[1/6] Loading application_metadata.csv...\")\n",
    "\n",
    "app_metadata = pd.read_csv(f\"{DATA_DIR}/application_metadata.csv\")\n",
    "\n",
    "print(f\"‚úì Shape: {app_metadata.shape}\")\n",
    "print(f\"‚úì Target variable 'default' distribution:\")\n",
    "print(f\"   Non-default (0): {(app_metadata['default'] == 0).sum():,} ({(app_metadata['default'] == 0).mean()*100:.2f}%)\")\n",
    "print(f\"   Default (1):     {(app_metadata['default'] == 1).sum():,} ({(app_metadata['default'] == 1).mean()*100:.2f}%)\")\n",
    "print(f\"\\n‚úì Sample:\")\n",
    "print(app_metadata.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 –ó–∞–≥—Ä—É–∑–∫–∞ demographics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[2/6] Loading demographics.csv...\")\n",
    "\n",
    "demographics = pd.read_csv(f\"{DATA_DIR}/demographics.csv\")\n",
    "\n",
    "print(f\"‚úì Shape: {demographics.shape}\")\n",
    "print(f\"\\n‚ö†Ô∏è Data Quality Issues Detected:\")\n",
    "print(f\"   annual_income formatting: {demographics['annual_income'].head(5).tolist()}\")\n",
    "print(f\"   employment_type inconsistency: {demographics['employment_type'].value_counts().head().to_dict()}\")\n",
    "print(f\"\\n‚úì Sample:\")\n",
    "print(demographics.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 –ó–∞–≥—Ä—É–∑–∫–∞ credit_history.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[3/6] Loading credit_history.parquet...\")\n",
    "\n",
    "credit_history = pd.read_parquet(f\"{DATA_DIR}/credit_history.parquet\")\n",
    "\n",
    "print(f\"‚úì Shape: {credit_history.shape}\")\n",
    "print(f\"‚úì Missing values: {credit_history.isnull().sum().sum()}\")\n",
    "print(f\"\\n‚úì Sample:\")\n",
    "print(credit_history.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 –ó–∞–≥—Ä—É–∑–∫–∞ financial_ratios.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[4/6] Loading financial_ratios.jsonl...\")\n",
    "\n",
    "financial_ratios = []\n",
    "with open(f\"{DATA_DIR}/financial_ratios.jsonl\", 'r') as f:\n",
    "    for line in f:\n",
    "        financial_ratios.append(json.loads(line))\n",
    "financial_ratios = pd.DataFrame(financial_ratios)\n",
    "\n",
    "print(f\"‚úì Shape: {financial_ratios.shape}\")\n",
    "print(f\"\\n‚ö†Ô∏è Data Quality Issues Detected:\")\n",
    "print(f\"   monthly_income formatting: {financial_ratios['monthly_income'].head(5).tolist()}\")\n",
    "print(f\"   (Contains $, commas - need cleaning)\")\n",
    "print(f\"\\n‚úì Sample:\")\n",
    "print(financial_ratios.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 –ó–∞–≥—Ä—É–∑–∫–∞ loan_details.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[5/6] Loading loan_details.xlsx...\")\n",
    "\n",
    "loan_details = pd.read_excel(f\"{DATA_DIR}/loan_details.xlsx\")\n",
    "\n",
    "print(f\"‚úì Shape: {loan_details.shape}\")\n",
    "print(f\"\\n‚úì Sample:\")\n",
    "print(loan_details.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 –ó–∞–≥—Ä—É–∑–∫–∞ geographic_data.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[6/6] Loading geographic_data.xml...\")\n",
    "\n",
    "tree = ET.parse(f\"{DATA_DIR}/geographic_data.xml\")\n",
    "root = tree.getroot()\n",
    "\n",
    "# Parse XML records\n",
    "geo_records = []\n",
    "for customer in root.findall('customer'):\n",
    "    record = {}\n",
    "    for child in customer:\n",
    "        record[child.tag] = child.text\n",
    "    geo_records.append(record)\n",
    "\n",
    "geographic_data = pd.DataFrame(geo_records)\n",
    "\n",
    "print(f\"‚úì Shape: {geographic_data.shape}\")\n",
    "print(f\"\\n‚úì Sample:\")\n",
    "print(geographic_data.head(3))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ALL 6 FILES LOADED SUCCESSFULLY\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ –û–ß–ò–°–¢–ö–ê –î–ê–ù–ù–´–•\n",
    "\n",
    "–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –∫–∞–∂–¥–æ–º —Ñ–∞–π–ª–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 –û—á–∏—Å—Ç–∫–∞ application_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Cleaning 1/6] application_metadata...\")\n",
    "\n",
    "# Remove noise column\n",
    "print(\"  Removing 'random_noise_1' column...\")\n",
    "app_metadata_clean = app_metadata.drop(columns=['random_noise_1'])\n",
    "\n",
    "print(f\"  ‚úì Shape after cleaning: {app_metadata_clean.shape}\")\n",
    "print(f\"  ‚úì Noise column removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 –û—á–∏—Å—Ç–∫–∞ demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Cleaning 2/6] demographics...\")\n",
    "\n",
    "demographics_clean = demographics.copy()\n",
    "\n",
    "# Clean annual_income (remove $, commas)\n",
    "print(\"  Cleaning annual_income (removing $, commas)...\")\n",
    "demographics_clean['annual_income'] = (\n",
    "    demographics_clean['annual_income']\n",
    "    .str.replace('$', '', regex=False)\n",
    "    .str.replace(',', '', regex=False)\n",
    "    .astype(float)\n",
    ")\n",
    "print(f\"    Before: {demographics['annual_income'].head(3).tolist()}\")\n",
    "print(f\"    After:  {demographics_clean['annual_income'].head(3).tolist()}\")\n",
    "\n",
    "# Normalize employment_type\n",
    "print(\"\\n  Normalizing employment_type...\")\n",
    "def normalize_employment_type(emp_type):\n",
    "    emp_upper = str(emp_type).upper()\n",
    "    if 'FULL' in emp_upper or 'FT' in emp_upper:\n",
    "        return 'Full-time'\n",
    "    elif 'PART' in emp_upper or 'PT' in emp_upper:\n",
    "        return 'Part-time'\n",
    "    elif 'SELF' in emp_upper or 'CONTRACT' in emp_upper:\n",
    "        return 'Self-employed'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "demographics_clean['employment_type'] = demographics_clean['employment_type'].apply(normalize_employment_type)\n",
    "print(f\"    Normalized categories: {demographics_clean['employment_type'].value_counts().to_dict()}\")\n",
    "\n",
    "# Impute missing employment_length\n",
    "missing_emp_len = demographics_clean['employment_length'].isnull().sum()\n",
    "if missing_emp_len > 0:\n",
    "    print(f\"\\n  Imputing {missing_emp_len} missing employment_length with 0...\")\n",
    "    demographics_clean['employment_length'] = demographics_clean['employment_length'].fillna(0)\n",
    "\n",
    "print(f\"\\n  ‚úì Demographics cleaned: {demographics_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 –û—á–∏—Å—Ç–∫–∞ credit_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Cleaning 3/6] credit_history...\")\n",
    "\n",
    "credit_history_clean = credit_history.copy()\n",
    "\n",
    "# Impute missing num_delinquencies_2yrs\n",
    "missing_delinq = credit_history_clean['num_delinquencies_2yrs'].isnull().sum()\n",
    "if missing_delinq > 0:\n",
    "    print(f\"  Imputing {missing_delinq} missing num_delinquencies_2yrs with 0...\")\n",
    "    credit_history_clean['num_delinquencies_2yrs'] = credit_history_clean['num_delinquencies_2yrs'].fillna(0)\n",
    "\n",
    "print(f\"  ‚úì Credit history cleaned: {credit_history_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 –û—á–∏—Å—Ç–∫–∞ financial_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Cleaning 4/6] financial_ratios...\")\n",
    "\n",
    "financial_ratios_clean = financial_ratios.copy()\n",
    "\n",
    "# Function to clean monetary fields\n",
    "def clean_monetary_field(series):\n",
    "    return (\n",
    "        series.astype(str)\n",
    "        .str.replace('$', '', regex=False)\n",
    "        .str.replace(',', '', regex=False)\n",
    "        .replace('nan', np.nan)\n",
    "        .astype(float)\n",
    "    )\n",
    "\n",
    "# Clean all monetary fields\n",
    "monetary_columns = ['monthly_income', 'existing_monthly_debt', 'monthly_payment',\n",
    "                   'revolving_balance', 'credit_usage_amount', 'available_credit',\n",
    "                   'total_monthly_debt_payment', 'total_debt_amount', 'monthly_free_cash_flow']\n",
    "\n",
    "print(\"  Cleaning monetary columns...\")\n",
    "for col in monetary_columns:\n",
    "    if col in financial_ratios_clean.columns:\n",
    "        print(f\"    {col}: \", end=\"\")\n",
    "        sample_before = str(financial_ratios_clean[col].iloc[0])\n",
    "        financial_ratios_clean[col] = clean_monetary_field(financial_ratios_clean[col])\n",
    "        sample_after = financial_ratios_clean[col].iloc[0]\n",
    "        print(f\"{sample_before} ‚Üí {sample_after}\")\n",
    "\n",
    "# Impute missing revolving_balance\n",
    "missing_revolv = financial_ratios_clean['revolving_balance'].isnull().sum()\n",
    "if missing_revolv > 0:\n",
    "    print(f\"\\n  Imputing {missing_revolv} missing revolving_balance with 0...\")\n",
    "    financial_ratios_clean['revolving_balance'] = financial_ratios_clean['revolving_balance'].fillna(0)\n",
    "\n",
    "print(f\"\\n  ‚úì Financial ratios cleaned: {financial_ratios_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 –û—á–∏—Å—Ç–∫–∞ loan_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Cleaning 5/6] loan_details...\")\n",
    "\n",
    "loan_details_clean = loan_details.copy()\n",
    "\n",
    "# Clean loan_amount if it has monetary formatting\n",
    "if loan_details_clean['loan_amount'].dtype == 'object':\n",
    "    print(\"  Cleaning loan_amount...\")\n",
    "    loan_details_clean['loan_amount'] = clean_monetary_field(loan_details_clean['loan_amount'])\n",
    "\n",
    "# Normalize loan_type\n",
    "print(\"  Normalizing loan_type...\")\n",
    "def normalize_loan_type(loan_type):\n",
    "    loan_upper = str(loan_type).upper()\n",
    "    if 'PERSONAL' in loan_upper:\n",
    "        return 'Personal'\n",
    "    elif 'MORTGAGE' in loan_upper or 'HOME' in loan_upper:\n",
    "        return 'Mortgage'\n",
    "    elif 'CREDIT' in loan_upper or 'CC' in loan_upper:\n",
    "        return 'Credit Card'\n",
    "    elif 'AUTO' in loan_upper or 'CAR' in loan_upper:\n",
    "        return 'Auto'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "loan_details_clean['loan_type'] = loan_details_clean['loan_type'].apply(normalize_loan_type)\n",
    "print(f\"    Categories: {loan_details_clean['loan_type'].value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"\\n  ‚úì Loan details cleaned: {loan_details_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 –û—á–∏—Å—Ç–∫–∞ geographic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Cleaning 6/6] geographic_data...\")\n",
    "\n",
    "geographic_data_clean = geographic_data.copy()\n",
    "\n",
    "# Convert numeric columns\n",
    "numeric_geo_cols = ['regional_unemployment_rate', 'regional_median_income',\n",
    "                   'regional_median_rent', 'housing_price_index', 'cost_of_living_index']\n",
    "\n",
    "print(\"  Converting to numeric...\")\n",
    "for col in numeric_geo_cols:\n",
    "    if col in geographic_data_clean.columns:\n",
    "        geographic_data_clean[col] = pd.to_numeric(geographic_data_clean[col], errors='coerce')\n",
    "        print(f\"    {col}: ‚úì\")\n",
    "\n",
    "print(f\"\\n  ‚úì Geographic data cleaned: {geographic_data_clean.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ALL DATA CLEANED SUCCESSFULLY\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –î–ê–¢–ê–°–ï–¢–û–í\n",
    "\n",
    "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö 6 —Ñ–∞–π–ª–æ–≤ –≤ –æ–¥–∏–Ω –¥–∞—Ç–∞—Å–µ—Ç."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MERGING ALL DATASETS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Start with application_metadata (contains target)\n",
    "merged = app_metadata_clean.copy()\n",
    "print(f\"[1] Starting with application_metadata: {merged.shape}\")\n",
    "\n",
    "# Merge demographics\n",
    "print(f\"[2] Merging demographics...\")\n",
    "merged = merged.merge(\n",
    "    demographics_clean,\n",
    "    left_on='customer_ref',\n",
    "    right_on='cust_id',\n",
    "    how='left',\n",
    "    validate='1:1'\n",
    ")\n",
    "print(f\"    After merge: {merged.shape}\")\n",
    "\n",
    "# Merge credit_history\n",
    "print(f\"[3] Merging credit_history...\")\n",
    "merged = merged.merge(\n",
    "    credit_history_clean,\n",
    "    left_on='customer_ref',\n",
    "    right_on='customer_number',\n",
    "    how='left',\n",
    "    validate='1:1'\n",
    ")\n",
    "print(f\"    After merge: {merged.shape}\")\n",
    "\n",
    "# Merge financial_ratios\n",
    "print(f\"[4] Merging financial_ratios...\")\n",
    "merged = merged.merge(\n",
    "    financial_ratios_clean,\n",
    "    left_on='customer_ref',\n",
    "    right_on='cust_num',\n",
    "    how='left',\n",
    "    validate='1:1'\n",
    ")\n",
    "print(f\"    After merge: {merged.shape}\")\n",
    "\n",
    "# Merge loan_details\n",
    "print(f\"[5] Merging loan_details...\")\n",
    "merged = merged.merge(\n",
    "    loan_details_clean,\n",
    "    left_on='customer_ref',\n",
    "    right_on='customer_id',\n",
    "    how='left',\n",
    "    validate='1:1'\n",
    ")\n",
    "print(f\"    After merge: {merged.shape}\")\n",
    "\n",
    "# Merge geographic_data\n",
    "print(f\"[6] Merging geographic_data...\")\n",
    "geographic_data_clean['id'] = geographic_data_clean['id'].astype(int)\n",
    "merged = merged.merge(\n",
    "    geographic_data_clean,\n",
    "    left_on='customer_ref',\n",
    "    right_on='id',\n",
    "    how='left',\n",
    "    validate='1:1'\n",
    ")\n",
    "print(f\"    After merge: {merged.shape}\")\n",
    "\n",
    "# Drop redundant ID columns\n",
    "print(f\"\\n[7] Dropping redundant ID columns...\")\n",
    "id_columns = ['cust_id', 'customer_number', 'cust_num', 'customer_id', 'id']\n",
    "merged = merged.drop(columns=[col for col in id_columns if col in merged.columns])\n",
    "print(f\"    Final shape: {merged.shape}\")\n",
    "\n",
    "print(\"\\n‚úÖ ALL DATASETS MERGED SUCCESSFULLY\")\n",
    "print(f\"   Total records: {merged.shape[0]:,}\")\n",
    "print(f\"   Total features: {merged.shape[1]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ –ò–ù–ñ–ï–ù–ï–†–ò–Ø –ü–†–ò–ó–ù–ê–ö–û–í\n",
    "\n",
    "–°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Income-based features\n",
    "print(\"[1] Creating income-based features...\")\n",
    "merged['monthly_income_from_annual'] = merged['annual_income'] / 12\n",
    "merged['disposable_income'] = merged['monthly_income'] - merged['existing_monthly_debt']\n",
    "merged['income_to_payment_capacity'] = merged['monthly_income'] / (merged['monthly_payment'] + 1)\n",
    "print(\"    ‚úì Created 3 features\")\n",
    "\n",
    "# Debt burden features\n",
    "print(\"[2] Creating debt burden features...\")\n",
    "merged['total_debt_to_income_annual'] = merged['total_debt_amount'] / (merged['annual_income'] + 1)\n",
    "merged['debt_payment_burden'] = (\n",
    "    (merged['existing_monthly_debt'] + merged['monthly_payment']) / (merged['monthly_income'] + 1)\n",
    ")\n",
    "merged['free_cash_flow_ratio'] = merged['monthly_free_cash_flow'] / (merged['monthly_income'] + 1)\n",
    "merged['loan_to_monthly_income'] = merged['loan_amount'] / (merged['monthly_income'] + 1)\n",
    "print(\"    ‚úì Created 4 features\")\n",
    "\n",
    "# Credit behavior features\n",
    "print(\"[3] Creating credit behavior features...\")\n",
    "merged['credit_age_to_score_ratio'] = merged['oldest_account_age_months'] / (merged['credit_score'] + 1)\n",
    "merged['delinquency_rate'] = merged['num_delinquencies_2yrs'] / (merged['num_credit_accounts'] + 1)\n",
    "merged['inquiry_intensity'] = merged['num_inquiries_6mo'] + merged['recent_inquiry_count']\n",
    "merged['negative_marks_total'] = (\n",
    "    merged['num_delinquencies_2yrs'].fillna(0) +\n",
    "    merged['num_public_records'] +\n",
    "    merged['num_collections']\n",
    ")\n",
    "merged['credit_stress_score'] = (\n",
    "    merged['credit_utilization'] * 0.3 +\n",
    "    merged['debt_to_income_ratio'] * 0.3 +\n",
    "    merged['delinquency_rate'] * 0.4\n",
    ")\n",
    "print(\"    ‚úì Created 5 features\")\n",
    "\n",
    "# Loan characteristics\n",
    "print(\"[4] Creating loan characteristic features...\")\n",
    "merged['loan_amount_to_limit'] = merged['loan_amount'] / (merged['total_credit_limit'] + 1)\n",
    "merged['interest_burden'] = merged['loan_amount'] * merged['interest_rate'] / 100\n",
    "merged['loan_term_years'] = merged['loan_term'] / 12\n",
    "merged['monthly_loan_payment_estimate'] = merged['loan_amount'] / (merged['loan_term'] + 1)\n",
    "print(\"    ‚úì Created 4 features\")\n",
    "\n",
    "# Regional economic features\n",
    "print(\"[5] Creating regional economic features...\")\n",
    "merged['income_to_regional_median'] = merged['annual_income'] / (merged['regional_median_income'] + 1)\n",
    "merged['housing_affordability'] = merged['regional_median_rent'] / (merged['monthly_income'] + 1)\n",
    "merged['regional_stress_index'] = (\n",
    "    merged['regional_unemployment_rate'] * 0.4 +\n",
    "    (merged['cost_of_living_index'] / 100) * 0.3 +\n",
    "    (merged['housing_price_index'] / 100) * 0.3\n",
    ")\n",
    "print(\"    ‚úì Created 3 features\")\n",
    "\n",
    "# Behavioral features\n",
    "print(\"[6] Creating behavioral features...\")\n",
    "merged['service_call_intensity'] = merged['num_customer_service_calls'] / (merged['num_login_sessions'] + 1)\n",
    "merged['digital_engagement_score'] = (\n",
    "    merged['has_mobile_app'] * 0.5 +\n",
    "    merged['paperless_billing'] * 0.3 +\n",
    "    (merged['num_login_sessions'] / merged['num_login_sessions'].max()) * 0.2\n",
    ")\n",
    "print(\"    ‚úì Created 2 features\")\n",
    "\n",
    "# Application timing features\n",
    "print(\"[7] Creating application timing features...\")\n",
    "merged['is_business_hours'] = ((merged['application_hour'] >= 9) & (merged['application_hour'] <= 17)).astype(int)\n",
    "merged['is_weekend'] = (merged['application_day_of_week'].isin([6, 7])).astype(int)\n",
    "merged['is_late_night'] = ((merged['application_hour'] >= 22) | (merged['application_hour'] <= 5)).astype(int)\n",
    "print(\"    ‚úì Created 3 features\")\n",
    "\n",
    "# Account maturity\n",
    "print(\"[8] Creating account maturity features...\")\n",
    "current_year = 2025\n",
    "merged['account_age_years'] = current_year - merged['account_open_year']\n",
    "merged['credit_history_depth'] = (\n",
    "    merged['oldest_credit_line_age'] * 0.5 +\n",
    "    merged['oldest_account_age_months'] * 0.5\n",
    ")\n",
    "print(\"    ‚úì Created 2 features\")\n",
    "\n",
    "print(f\"\\n‚úÖ FEATURE ENGINEERING COMPLETE\")\n",
    "print(f\"   Total new features: 26\")\n",
    "print(f\"   Dataset shape: {merged.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ –û–ë–†–ê–ë–û–¢–ö–ê –ü–†–û–ü–£–©–ï–ù–ù–´–• –ó–ù–ê–ß–ï–ù–ò–ô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HANDLING MISSING VALUES\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "missing_summary = merged.isnull().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_summary) > 0:\n",
    "    print(\"Columns with missing values:\")\n",
    "    for col, count in missing_summary.items():\n",
    "        pct = (count / len(merged)) * 100\n",
    "        print(f\"  {col}: {count:,} ({pct:.2f}%)\")\n",
    "    print(\"\\nImputing with 0 (conservative approach for financial data)...\")\n",
    "    merged = merged.fillna(0)\n",
    "    print(\"‚úì All missing values imputed\")\n",
    "else:\n",
    "    print(\"‚úì No missing values detected\")\n",
    "\n",
    "print(f\"\\nFinal shape: {merged.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ –†–ê–ó–î–ï–õ–ï–ù–ò–ï –ù–ê TRAIN/TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAIN/TEST SPLIT\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Separate features and target\n",
    "X = merged.drop(columns=['default'])\n",
    "y = merged['default']\n",
    "\n",
    "# Stratified split to preserve class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples ({(X_train.shape[0]/len(X))*100:.1f}%)\")\n",
    "print(f\"  Default rate: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"\\nTest set: {X_test.shape[0]:,} samples ({(X_test.shape[0]/len(X))*100:.1f}%)\")\n",
    "print(f\"  Default rate: {y_test.mean()*100:.2f}%\")\n",
    "print(f\"\\n‚úì Class balance preserved in both sets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ –ö–û–î–ò–†–û–í–ê–ù–ò–ï –ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–• –ü–ï–†–ï–ú–ï–ù–ù–´–•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENCODING CATEGORICAL VARIABLES\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Categorical columns: {len(categorical_cols)}\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Label encode (fit on combined train+test to handle all categories)\n",
    "print(\"\\nLabel encoding...\")\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    all_categories = pd.concat([X_train[col], X_test[col]]).astype(str).unique()\n",
    "    le = LabelEncoder()\n",
    "    le.fit(all_categories)\n",
    "    X_train[col] = le.transform(X_train[col].astype(str))\n",
    "    X_test[col] = le.transform(X_test[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"  {col}: {len(le.classes_)} classes ‚Üí [0-{len(le.classes_)-1}]\")\n",
    "\n",
    "print(\"\\n‚úì All categorical variables encoded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü –°–û–•–†–ê–ù–ï–ù–ò–ï –î–ê–¢–ê–°–ï–¢–û–í"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING DATASETS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Save cleaned datasets\n",
    "X_train.to_parquet('X_train_optimized.parquet', index=False)\n",
    "X_test.to_parquet('X_test_optimized.parquet', index=False)\n",
    "pd.DataFrame({'default': y_train}).to_parquet('y_train.parquet', index=False)\n",
    "pd.DataFrame({'default': y_test}).to_parquet('y_test.parquet', index=False)\n",
    "\n",
    "print(\"‚úì Saved files:\")\n",
    "print(f\"  - X_train_optimized.parquet: {X_train.shape}\")\n",
    "print(f\"  - X_test_optimized.parquet: {X_test.shape}\")\n",
    "print(f\"  - y_train.parquet: {len(y_train):,} records\")\n",
    "print(f\"  - y_test.parquet: {len(y_test):,} records\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ DATA CLEANING AND MERGING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Download the 4 parquet files\")\n",
    "print(\"  2. Upload to Data_Preparation_Colab_V3.ipynb\")\n",
    "print(\"  3. Create leak-free OOF features\")\n",
    "print(\"  4. Train XGBoost model\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Data Summary:\")\n",
    "print(f\"  Total records processed: {merged.shape[0]:,}\")\n",
    "print(f\"  Total features created: {merged.shape[1]:,}\")\n",
    "print(f\"  Training samples: {X_train.shape[0]:,}\")\n",
    "print(f\"  Test samples: {X_test.shape[0]:,}\")\n",
    "print(f\"  Class imbalance ratio: {(y_train == 0).sum() / (y_train == 1).sum():.1f}:1\")\n",
    "\n",
    "print(f\"\\nüîß Feature Engineering:\")\n",
    "print(f\"  Income-based: 3 features\")\n",
    "print(f\"  Debt burden: 4 features\")\n",
    "print(f\"  Credit behavior: 5 features\")\n",
    "print(f\"  Loan characteristics: 4 features\")\n",
    "print(f\"  Regional economic: 3 features\")\n",
    "print(f\"  Behavioral: 2 features\")\n",
    "print(f\"  Application timing: 3 features\")\n",
    "print(f\"  Account maturity: 2 features\")\n",
    "print(f\"  TOTAL ENGINEERED: 26 features\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data Quality:\")\n",
    "print(f\"  ‚úì No missing values\")\n",
    "print(f\"  ‚úì No noise columns\")\n",
    "print(f\"  ‚úì All monetary fields cleaned\")\n",
    "print(f\"  ‚úì Categorical variables normalized\")\n",
    "print(f\"  ‚úì Class balance preserved in split\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Download Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"üì• Downloading files...\\n\")\n",
    "\n",
    "files.download('X_train_optimized.parquet')\n",
    "files.download('X_test_optimized.parquet')\n",
    "files.download('y_train.parquet')\n",
    "files.download('y_test.parquet')\n",
    "\n",
    "print(\"\\n‚úÖ All files downloaded!\")\n",
    "print(\"\\nüéØ Use these files in Data_Preparation_Colab_V3.ipynb for OOF feature engineering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Summary\n",
    "\n",
    "### Completed Tasks:\n",
    "1. ‚úÖ Loaded 6 heterogeneous data files (CSV, Parquet, Excel, XML, JSONL)\n",
    "2. ‚úÖ Cleaned all data quality issues:\n",
    "   - Removed `random_noise_1` column\n",
    "   - Cleaned monetary formatting ($, commas)\n",
    "   - Normalized categorical variables\n",
    "   - Imputed missing values\n",
    "3. ‚úÖ Merged all datasets into single unified file\n",
    "4. ‚úÖ Created 26 engineered features\n",
    "5. ‚úÖ Performed stratified 80/20 train/test split\n",
    "6. ‚úÖ Encoded categorical variables\n",
    "7. ‚úÖ Saved cleaned datasets\n",
    "\n",
    "### Output Files:\n",
    "- `X_train_optimized.parquet` - Clean training features\n",
    "- `X_test_optimized.parquet` - Clean test features\n",
    "- `y_train.parquet` - Training labels\n",
    "- `y_test.parquet` - Test labels\n",
    "\n",
    "### Next Steps:\n",
    "1. Use `Data_Preparation_Colab_V3.ipynb` to create leak-free OOF features\n",
    "2. Use `Google_Colab_Leak_Free_90plus_v3.ipynb` to train XGBoost model\n",
    "3. Achieve 90%+ AUC!\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Data Pipeline Engineer  \n",
    "**Version:** V1 - Data Cleaning & Merging  \n",
    "**Date:** November 2025\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
