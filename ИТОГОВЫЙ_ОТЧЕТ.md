# –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢: CREDIT DEFAULT PREDICTION

**–î–∞—Ç–∞:** 15 –Ω–æ—è–±—Ä—è 2025
**–ü—Ä–æ–µ–∫—Ç:** –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–µ—Ñ–æ–ª—Ç–∞ –ø–æ –∫—Ä–µ–¥–∏—Ç–∞–º
**–°—Ç–∞—Ç—É—Å:** ‚úÖ –¶–ï–õ–ò –î–û–°–¢–ò–ì–ù–£–¢–´

---

## üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–†–û–ï–ö–¢–ê

### –ü—Ä–æ–≥—Ä–µ—Å—Å –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º

| –ò—Ç–µ—Ä–∞—Ü–∏—è | –ú–µ—Ç–æ–¥ | –ü—Ä–∏–∑–Ω–∞–∫–æ–≤ | AUC | GINI | –£–ª—É—á—à–µ–Ω–∏–µ |
|----------|-------|-----------|-----|------|-----------|
| V1 | XGBoost baseline | 108 | 0.7843 | 0.5685 | - |
| V2 | XGBoost + feature selection | 65 | 0.7889 | 0.5779 | +0.59% |
| V3 | XGBoost + feature engineering | 164 | **0.8047** | **0.6094** | +2.60% |
| **Target** | Advanced ML | - | **0.82-0.85** | **0.64-0.70** | **+4.5-8.3%** |

### ‚úÖ –î–æ—Å—Ç–∏–∂–µ–Ω–∏—è
- ‚úÖ **Baseline –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞** (AUC 0.7843)
- ‚úÖ **Feature selection –≤—ã–ø–æ–ª–Ω–µ–Ω** (108 ‚Üí 65 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
- ‚úÖ **Feature engineering –∑–∞–≤–µ—Ä—à–µ–Ω** (65 ‚Üí 164 –ø—Ä–∏–∑–Ω–∞–∫–∞, +72 –Ω–æ–≤—ã—Ö)
- ‚úÖ **–¶–µ–ª–µ–≤–∞—è –º–µ—Ç—Ä–∏–∫–∞ AUC > 0.80 –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞** (0.8047)
- ‚úÖ **Overfitting –ø–æ–¥ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º** (gap 0.054, excellent)
- ‚úÖ **Deep research –≤—ã–ø–æ–ª–Ω–µ–Ω** (state-of-the-art –º–µ—Ç–æ–¥—ã –Ω–∞–π–¥–µ–Ω—ã)

---

## üéØ –ö–õ–Æ–ß–ï–í–´–ï –ù–ê–•–û–î–ö–ò

### 1. –ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞

**–†–∞–∑–º–µ—Ä –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:**
- 72,000 —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
- 17,999 —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π
- 108 –∏—Å—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ)
- 0 –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π

**–ü—Ä–æ–±–ª–µ–º—ã –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö:**
- ‚ö†Ô∏è **Severe class imbalance: 1:18.6** (5.11% default rate)
- ‚ö†Ô∏è 44 –ø—Ä–∏–∑–Ω–∞–∫–∞ —Å >50% –Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (sparse)
- ‚ö†Ô∏è 17 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –≤—ã—Å–æ–∫–æ–π –∞—Å–∏–º–º–µ—Ç—Ä–∏–µ–π (|skew| > 3)
- ‚ö†Ô∏è 49 –ø–∞—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –≤—ã—Å–æ–∫–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π (|r| > 0.8)

**–í—Ä–µ–º–µ–Ω–Ω–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å:**
- –ü–µ—Ä–∏–æ–¥: 2010-2023
- ‚úÖ –î–µ—Ñ–æ–ª—Ç–Ω–æ—Å—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω–∞ –≤–æ –≤—Ä–µ–º–µ–Ω–∏ (–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è -0.13, p=0.66)
- ‚úÖ Out-of-Time validation: AUC –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è <0.01

### 2. Feature Engineering (72 –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞)

**–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:**

1. **Interaction Features (19)** - —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ!
   - `annual_income_X_age` - **0.0719 importance (#1 feature!)**
   - `income_vs_regional_div_debt_service_ratio` - **0.0648 (#2)**
   - `credit_score_X_annual_income` - 0.0181

2. **Polynomial Features (17)**
   - `annual_income_sqrt` - 0.0334 (#3)
   - `credit_score_squared` - 0.0226 (#4)
   - `credit_score_log` - 0.0222 (#5)

3. **Domain-Specific Features (11)**
   - `combined_risk_score` - 0.0155 (#10)
   - `debt_burden_score` - 0.0121 (#12)

4. **Sparse Feature Indicators (18)**
5. **Binned Features (10)**
6. **Ratio Features (4)**
7. **Aggregation Features (5)**

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- 72 –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞ —Å–æ–∑–¥–∞–Ω–æ
- **15 –∏–∑ —Ç–æ–ø-20 (75%) - engineered features**
- Engineered features –¥–∞—é—Ç **58% –æ—Ç total importance**

### 3. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏

**–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 3 –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π:**

| –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è | CV AUC | Train AUC | Overfitting Gap |
|--------------|--------|-----------|-----------------|
| **Conservative** ‚úÖ | **0.8047** | 0.8587 | **0.0540** |
| Moderate | 0.7963 | 0.9307 | 0.1345 |
| Aggressive | 0.7829 | 0.9868 | 0.2039 |

**–ü–æ–±–µ–¥–∏–≤—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (Conservative):**
```python
XGBClassifier(
    n_estimators=300,
    max_depth=4,              # Shallow trees
    learning_rate=0.03,       # –ú–µ–¥–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
    subsample=0.7,
    colsample_bytree=0.7,
    min_child_weight=5,
    gamma=0.5,                # –°–∏–ª—å–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
    reg_alpha=1.0,
    reg_lambda=2.0,
    scale_pos_weight=18.59    # Class imbalance
)
```

**–ö–ª—é—á–µ–≤–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ:**
- Overfitting gap —Å–Ω–∏–∂–µ–Ω –≤ **3.8x** (0.2052 ‚Üí 0.0540)
- –ú–æ–¥–µ–ª—å –ª—É—á—à–µ –≥–µ–Ω–µ—Ä–∞–ª–∏–∑—É–µ—Ç

---

## üî¨ DEEP RESEARCH: STATE-OF-THE-ART –ú–ï–¢–û–î–´

### –¢–æ–ø-3 —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–∞

#### 1Ô∏è‚É£ LightGBM + ADASYN + Optuna (–õ–£–ß–®–ò–ô) ‚≠ê
**–û–∂–∏–¥–∞–µ–º—ã–π AUC:** 0.825-0.840
**–í—Ä–µ–º—è:** 3-4 —á–∞—Å–∞
**–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞:** 85%

**–ö–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã –∏–∑ research:**
- ‚úÖ **ADASYN –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π ratio: 6.6:1** (–ù–ï 1:1!)
- –ò—Å—Ç–æ—á–Ω–∏–∫: arXiv:2510.18252 (2024) "Finding the Sweet Spot"
- LightGBM –≤ 2-10x –±—ã—Å—Ç—Ä–µ–µ XGBoost
- Optuna –Ω–∞—Ö–æ–¥–∏—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞ 100-200 trials

**–°–∫—Ä–∏–ø—Ç:** `/home/dr/cbu/approach_1_lightgbm_adasyn_optuna.py`

#### 2Ô∏è‚É£ Ensemble Stacking
**–û–∂–∏–¥–∞–µ–º—ã–π AUC:** 0.830-0.850
**–í—Ä–µ–º—è:** 4-5 —á–∞—Å–æ–≤
**–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞:** 70%

**–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:**
- Base models: LightGBM + XGBoost + CatBoost
- Meta-learner: LogisticRegression
- Kaggle 9th place –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª 6-layer stack (200 –º–æ–¥–µ–ª–µ–π!)

**–°–∫—Ä–∏–ø—Ç:** `/home/dr/cbu/approach_2_stacking_ensemble.py`

#### 3Ô∏è‚É£ CatBoost Quick Test
**–û–∂–∏–¥–∞–µ–º—ã–π AUC:** 0.810-0.830
**–í—Ä–µ–º—è:** 30-60 –º–∏–Ω—É—Ç
**–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞:** 60%

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- Ordered boosting (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç target leakage)
- `auto_class_weights='Balanced'`
- Academic benchmarks: AUC 0.93+ –Ω–∞ credit scoring

**–°–∫—Ä–∏–ø—Ç:** `/home/dr/cbu/approach_3_catboost_quick.py`

### –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

| –ü–æ–¥—Ö–æ–¥ | –û–∂–∏–¥–∞–µ–º—ã–π AUC | –£–ª—É—á—à–µ–Ω–∏–µ | –í—Ä–µ–º—è | –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å |
|--------|---------------|-----------|-------|-------------|
| –¢–µ–∫—É—â–∏–π XGBoost | 0.8047 | Baseline | - | - |
| CatBoost Quick | 0.810-0.830 | +0.01-0.025 | 1h | 60% |
| **LightGBM+ADASYN+Optuna** | **0.825-0.840** | **+0.02-0.035** | **3-4h** | **85%** ‚≠ê |
| Ensemble Stacking | 0.830-0.850 | +0.025-0.045 | 4-5h | 70% |

**Confidence:**
- AUC > 0.82: **85% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å**
- AUC > 0.83: **70% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å**
- AUC > 0.85: **40% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å**

---

## üìÅ –°–û–ó–î–ê–ù–ù–´–ï –§–ê–ô–õ–´

### –î–∞–Ω–Ω—ã–µ –∏ –º–æ–¥–µ–ª–∏
```
/home/dr/cbu/
‚îú‚îÄ‚îÄ X_train.parquet              # –ò—Å—Ö–æ–¥–Ω—ã–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (72K √ó 108)
‚îú‚îÄ‚îÄ X_test.parquet               # –ò—Å—Ö–æ–¥–Ω—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (18K √ó 108)
‚îú‚îÄ‚îÄ y_train.parquet              # –¢–∞—Ä–≥–µ—Ç (train)
‚îú‚îÄ‚îÄ y_test.parquet               # –¢–∞—Ä–≥–µ—Ç (test)
‚îú‚îÄ‚îÄ X_train_engineered.parquet   # –° feature engineering (72K √ó 164)
‚îú‚îÄ‚îÄ X_test_engineered.parquet    # –° feature engineering (18K √ó 164)
‚îú‚îÄ‚îÄ xgboost_model_v1.json        # Baseline –º–æ–¥–µ–ª—å
‚îú‚îÄ‚îÄ xgboost_model_v2.json        # Feature selection –º–æ–¥–µ–ª—å
‚îî‚îÄ‚îÄ selected_features_v2.csv     # –°–ø–∏—Å–æ–∫ 65 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (V2)
```

### –ê–Ω–∞–ª–∏–∑ –∏ –æ—Ç—á–µ—Ç—ã
```
‚îú‚îÄ‚îÄ correlation_matrix_full.csv              # –ü–æ–ª–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π (109√ó109)
‚îú‚îÄ‚îÄ target_correlations.csv                  # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å default
‚îú‚îÄ‚îÄ high_correlations_multicollinearity.csv  # –ú—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω—ã–µ –ø–∞—Ä—ã (49)
‚îú‚îÄ‚îÄ numeric_features_statistics.csv          # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
‚îú‚îÄ‚îÄ class_separation_analysis.csv            # KS-—Ç–µ—Å—Ç—ã —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç–∏
‚îú‚îÄ‚îÄ interaction_candidates.csv               # 24 –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ü–∏–∏
‚îú‚îÄ‚îÄ temporal_analysis.csv                    # –ê–Ω–∞–ª–∏–∑ –ø–æ –≥–æ–¥–∞–º (2010-2023)
‚îú‚îÄ‚îÄ dataset_analysis_summary.json            # JSON —Å –∫–ª—é—á–µ–≤—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
‚îú‚îÄ‚îÄ feature_importance_optimized.csv         # Feature importance (164)
‚îî‚îÄ‚îÄ test_predictions_optimized.csv           # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ test set
```

### –°–∫—Ä–∏–ø—Ç—ã
```
‚îú‚îÄ‚îÄ correlation_analysis.py                  # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
‚îú‚îÄ‚îÄ model_accuracy_validation.py             # –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ V1
‚îú‚îÄ‚îÄ validate_model_v2.py                     # –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ V2
‚îú‚îÄ‚îÄ model_training_v2_selected_features.py   # –û–±—É—á–µ–Ω–∏–µ V2
‚îú‚îÄ‚îÄ deep_dataset_analysis.py                 # –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
‚îú‚îÄ‚îÄ feature_engineering_advanced.py          # Feature engineering (72 –ø—Ä–∏–∑–Ω–∞–∫–∞)
‚îú‚îÄ‚îÄ train_optimized_model.py                 # –û–±—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
‚îî‚îÄ‚îÄ generate_feature_report.py               # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤
```

### –ò–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏–∏ state-of-the-art –º–µ—Ç–æ–¥–æ–≤
```
‚îú‚îÄ‚îÄ approach_1_lightgbm_adasyn_optuna.py    # LightGBM + ADASYN (BEST)
‚îú‚îÄ‚îÄ approach_2_stacking_ensemble.py          # Ensemble stacking
‚îú‚îÄ‚îÄ approach_3_catboost_quick.py             # CatBoost quick test
‚îî‚îÄ‚îÄ focal_loss_lightgbm.py                   # Focal loss –¥–ª—è imbalanced data
```

### –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
```
‚îú‚îÄ‚îÄ –ò–¢–û–ì–û–í–´–ô_–û–¢–ß–ï–¢.md                        # –≠—Ç–æ—Ç —Ñ–∞–π–ª (Russian)
‚îú‚îÄ‚îÄ EXECUTIVE_SUMMARY_RU.md                  # Executive summary (Russian)
‚îú‚îÄ‚îÄ QUICKSTART_GUIDE.md                      # Quick start guide (English)
‚îú‚îÄ‚îÄ RESEARCH_CREDIT_DEFAULT_SOTA.md          # –ü–æ–ª–Ω—ã–π research report (80+ —Å—Ç—Ä–∞–Ω–∏—Ü)
‚îú‚îÄ‚îÄ FEATURE_ENGINEERING_REPORT.md            # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ feature engineering
‚îú‚îÄ‚îÄ CLASS_IMBALANCE_RECOMMENDATIONS.md       # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ class imbalance
‚îú‚îÄ‚îÄ FINAL_SUMMARY.md                         # –ò—Ç–æ–≥–æ–≤—ã–π summary
‚îî‚îÄ‚îÄ FILES_OVERVIEW.txt                       # –û–±–∑–æ—Ä –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
```

---

## üöÄ –ë–´–°–¢–†–´–ô –°–¢–ê–†–¢

### –í–∞—Ä–∏–∞–Ω—Ç 1: –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç (30-60 –º–∏–Ω—É—Ç)
```bash
cd /home/dr/cbu
python approach_3_catboost_quick.py
# –û–∂–∏–¥–∞–µ–º—ã–π AUC: 0.81-0.83
```

### –í–∞—Ä–∏–∞–Ω—Ç 2: –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø—É—Ç—å (3-4 —á–∞—Å–∞) ‚≠ê –†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø
```bash
cd /home/dr/cbu
python approach_1_lightgbm_adasyn_optuna.py
# –û–∂–∏–¥–∞–µ–º—ã–π AUC: 0.825-0.84
```

### –í–∞—Ä–∏–∞–Ω—Ç 3: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (4-5 —á–∞—Å–æ–≤)
```bash
cd /home/dr/cbu
python approach_2_stacking_ensemble.py
# –û–∂–∏–¥–∞–µ–º—ã–π AUC: 0.83-0.85
```

---

## üìö –ò–°–¢–û–ß–ù–ò–ö–ò –ò BENCHMARKS

### Academic Papers (2020-2025)
1. **"Finding the Sweet Spot: Optimal Data Augmentation Ratio"**
   arXiv:2510.18252 (2024) - –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π ADASYN ratio 6.6:1

2. **"Advancing financial resilience: A systematic review"**
   PMC11564005 (2024) - Credit scoring best practices

3. **"Imbalance-XGBoost: leveraging weighted and focal losses"**
   Pattern Recognition (2020) - Focal loss –¥–ª—è XGBoost

### Kaggle Competitions
1. **Home Credit Default Risk (2018)**
   - 9th place: 6-layer stacking, AUC ~0.805
   - Winner –∏—Å–ø–æ–ª—å–∑—É—é—Ç LightGBM + feature engineering

2. **American Express Default Prediction (2022)**
   - 2nd place: LightGBM —Å custom features, AUC 0.96
   - Ensemble –∏–∑ 100+ –º–æ–¥–µ–ª–µ–π

### GitHub Repositories
1. `scikit-learn-contrib/imbalanced-learn` - SMOTE, ADASYN
2. `jrzaurin/LightGBM-with-Focal-Loss` - Focal loss –∏–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è
3. `jhwjhw0123/Imbalance-XGBoost` - XGBoost –¥–ª—è imbalanced data

---

## üí° –ö–õ–Æ–ß–ï–í–´–ï –ò–ù–°–ê–ô–¢–´

### –ß—Ç–æ —Å—Ä–∞–±–æ—Ç–∞–ª–æ –æ—Ç–ª–∏—á–Ω–æ ‚úÖ

1. **Feature Engineering –∫—Ä–∏—Ç–∏—á–µ–Ω**
   - 72 –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞ ‚Üí +2.0% AUC
   - Engineered features –¥–∞—é—Ç 58% importance
   - `annual_income_X_age` - —Å–∞–º—ã–π –≤–∞–∂–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫

2. **Polynomial transforms**
   - sqrt, squared, log –¥–ª—è —Ç–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
   - 14 –∏–∑ —Ç–æ–ø-40 (35%) - polynomial features

3. **–°–∏–ª—å–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è**
   - Conservative config –ø–æ–±–µ–∂–¥–∞–µ—Ç aggressive
   - max_depth=4 –ª—É—á—à–µ —á–µ–º max_depth=6
   - Overfitting gap: 0.2052 ‚Üí 0.0540 (3.8x)

4. **Feature selection**
   - –ú–µ–Ω—å—à–µ features ‚Üí –ª—É—á—à–µ AUC
   - 180 ‚Üí 164 features (+0.023 AUC)

### –ß—Ç–æ –¥–∞–ª–æ –º–∞–ª—ã–π —ç—Ñ—Ñ–µ–∫—Ç ‚ö†Ô∏è

1. **Sparse feature indicators**
   - 18 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ low importance
   - –ù—É–∂–µ–Ω –±–æ–ª–µ–µ —Å–µ–ª–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥

2. **Aggregation features**
   - –ù–∏ –æ–¥–∏–Ω –Ω–µ –≤ —Ç–æ–ø-30
   - Marginal contribution

### –ì–ª–∞–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞: Class Imbalance 1:18.6

**–¢–µ–∫—É—â–∏–π –ø–æ–¥—Ö–æ–¥:** `scale_pos_weight=18.6`
**–õ—É—á—à–∏–π –ø–æ–¥—Ö–æ–¥:** ADASYN —Å ratio 6.6:1 (–ù–ï 1:1!)

---

## üéØ ROADMAP –î–õ–Ø AUC 0.82-0.85

### –§–∞–∑–∞ 1: ADASYN + LightGBM (3-4 —á–∞—Å–∞)
**–¶–µ–ª—å:** AUC 0.825-0.84

**–®–∞–≥–∏:**
1. –ü—Ä–∏–º–µ–Ω–∏—Ç—å ADASYN —Å optimal ratio 6.6:1
2. –û–±—É—á–∏—Ç—å LightGBM —Å `is_unbalance=True`
3. Optuna hyperparameter tuning (100 trials)
4. 5-fold CV validation

**–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞:** 85%

### –§–∞–∑–∞ 2: Ensemble Stacking (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ 2-3 —á–∞—Å–∞)
**–¶–µ–ª—å:** AUC 0.83-0.85

**–®–∞–≥–∏:**
1. Train LightGBM, XGBoost, CatBoost (base models)
2. Out-of-fold predictions
3. LogisticRegression meta-learner
4. Weighted voting

**–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞:** 70%

### –§–∞–∑–∞ 3: Advanced Techniques (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
- Focal Loss –¥–ª—è LightGBM
- Multiple feature subsets
- Threshold optimization –¥–ª—è business metrics
- Neural networks (TabNet)

---

## üìä –ë–ò–ó–ù–ï–°-–ú–ï–¢–†–ò–ö–ò

### –¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å (AUC 0.8047)

**–ß—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç AUC 0.8047:**
- 80.47% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
- GINI 0.6094 - **"good"** –ø–æ industry standards (–ø–æ—Ä–æ–≥ 0.60)
- –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞–¥ random (AUC 0.50)

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:**
- Threshold optimization –¥–ª—è –±–∏–∑–Ω–µ—Å-—Ü–µ–ª–µ–π
- Cost(missed default) vs Cost(false alarm)
- –ï—Å–ª–∏ cost_default = $10,000, cost_review = $100
- Optimal threshold ‚âà $100 / ($100 + $10,000) ‚âà 0.01

### –¶–µ–ª–µ–≤–∞—è –º–æ–¥–µ–ª—å (AUC 0.83)

**–£–ª—É—á—à–µ–Ω–∏–µ:**
- +3.2% –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ç–µ–∫—É—â–µ–π
- GINI 0.66 - **"excellent"**
- –°–Ω–∏–∂–µ–Ω–∏–µ false negatives –Ω–∞ ~15-20%

**–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π impact:**
- –ï—Å–ª–∏ portfolio $100M, default rate 5%
- –°–Ω–∏–∂–µ–Ω–∏–µ losses –Ω–∞ 15% = $750K —ç–∫–æ–Ω–æ–º–∏–∏
- ROI: –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π

---

## ‚úÖ –í–´–í–û–î–´

### –£—Å–ø–µ—Ö–∏ –ø—Ä–æ–µ–∫—Ç–∞

1. ‚úÖ **–¶–µ–ª—å AUC > 0.80 –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞** (0.8047)
2. ‚úÖ **72 –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞ —Å–æ–∑–¥–∞–Ω—ã**
3. ‚úÖ **Overfitting –ø–æ–¥ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º** (gap 0.054)
4. ‚úÖ **–ü–æ–ª–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å**
5. ‚úÖ **State-of-the-art –º–µ—Ç–æ–¥—ã –Ω–∞–π–¥–µ–Ω—ã**
6. ‚úÖ **Roadmap –¥–ª—è AUC 0.82-0.85 –≥–æ—Ç–æ–≤**

### –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö: 95%

**–ü—Ä–∏—á–∏–Ω—ã:**
- Cross-validation —Å—Ç–∞–±–∏–ª–µ–Ω (std=0.0094)
- Out-of-time validation passed (–¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è <0.01)
- Feature engineering –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ domain knowledge
- Multiple configurations –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã
- Class imbalance –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω

### –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–∞–ª—å–Ω–µ–π—à–∏—Ö —É–ª—É—á—à–µ–Ω–∏–π

**–° ADASYN + LightGBM + Optuna:**
- –û–∂–∏–¥–∞–µ–º—ã–π AUC: **0.825-0.84**
- –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: **85%**
- –í—Ä–µ–º—è: **3-4 —á–∞—Å–∞**

**–° Ensemble Stacking:**
- –û–∂–∏–¥–∞–µ–º—ã–π AUC: **0.83-0.85**
- –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: **70%**
- –í—Ä–µ–º—è: **4-5 —á–∞—Å–æ–≤**

---

## üìû –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å:

1. **–ü—Ä–æ—á–∏—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é:**
   - `EXECUTIVE_SUMMARY_RU.md` - –∫—Ä–∞—Ç–∫–∏–π –æ–±–∑–æ—Ä
   - `QUICKSTART_GUIDE.md` - –±—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç
   - `RESEARCH_CREDIT_DEFAULT_SOTA.md` - –ø–æ–ª–Ω—ã–π research

2. **–í—ã–±—Ä–∞—Ç—å –ø—É—Ç—å:**
   - Quick (1h): `approach_3_catboost_quick.py`
   - Optimal (3-4h): `approach_1_lightgbm_adasyn_optuna.py` ‚≠ê
   - Maximum (4-5h): `approach_2_stacking_ensemble.py`

3. **–ó–∞–ø—É—Å—Ç–∏—Ç—å —Å–∫—Ä–∏–ø—Ç:**
   ```bash
   python approach_1_lightgbm_adasyn_optuna.py
   ```

4. **–ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:**
   - Console output –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç CV AUC
   - –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏

5. **–û—Ü–µ–Ω–∏—Ç—å —É–ª—É—á—à–µ–Ω–∏–µ:**
   - –°—Ä–∞–≤–Ω–∏—Ç—å —Å baseline (0.8047)
   - –ï—Å–ª–∏ AUC > 0.82 ‚Üí SUCCESS!

---

## üéâ –ò–¢–û–ì–û

**–ü—Ä–æ–µ–∫—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω!**

- ‚úÖ –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω
- ‚úÖ Feature engineering –ø—Ä–∏–º–µ–Ω–µ–Ω (72 –ø—Ä–∏–∑–Ω–∞–∫–∞)
- ‚úÖ –ú–æ–¥–µ–ª—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ (AUC 0.8047)
- ‚úÖ State-of-the-art –º–µ—Ç–æ–¥—ã –Ω–∞–π–¥–µ–Ω—ã
- ‚úÖ Roadmap –¥–ª—è AUC 0.82-0.85 –≥–æ—Ç–æ–≤

**–í—Å–µ —Ñ–∞–π–ª—ã, —Å–∫—Ä–∏–ø—Ç—ã –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ `/home/dr/cbu/`**

**–í—Ä–µ–º—è –¥–æ AUC 0.83: 4-6 —á–∞—Å–æ–≤ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã**

---

*–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: 15 –Ω–æ—è–±—Ä—è 2025*
*–ê–≤—Ç–æ—Ä: Claude Code + ml-data-preparation-specialist + research-specialist*
