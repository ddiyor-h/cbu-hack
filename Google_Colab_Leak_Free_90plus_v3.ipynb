{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üéØ XGBoost Leak-Free 90%+ AUC Pipeline V3\n",
    "\n",
    "## ‚ö†Ô∏è –í–ê–ñ–ù–û: –≠—Ç–æ—Ç notebook –∏—Å–ø–æ–ª—å–∑—É–µ—Ç leak-free –¥–∞–Ω–Ω—ã–µ V3!\n",
    "\n",
    "### –ö–ª—é—á–µ–≤—ã–µ –æ—Ç–ª–∏—á–∏—è –æ—Ç V2:\n",
    "- ‚úÖ **–£—Å—Ç—Ä–∞–Ω–µ–Ω–∞ —É—Ç–µ—á–∫–∞ –¥–∞–Ω–Ω—ã—Ö** - –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ–∑–¥–∞–Ω—ã out-of-fold\n",
    "- ‚úÖ **–ß–µ—Å—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞** - test AUC –±—É–¥–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–º\n",
    "- ‚úÖ **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è 90%+ AUC** - –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã + –∞–Ω—Å–∞–º–±–ª—å\n",
    "\n",
    "### –ß—Ç–æ –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å:\n",
    "1. `X_train_leak_free_v3.parquet` (71,999 √ó 89)\n",
    "2. `X_test_leak_free_v3.parquet` (18,000 √ó 89)\n",
    "3. `y_train_leak_free_v3.parquet`\n",
    "4. `y_test_leak_free_v3.parquet`\n",
    "\n",
    "### –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:\n",
    "- –° 50 trials: ~15-20 –º–∏–Ω—É—Ç\n",
    "- –° 30 trials: ~10-15 –º–∏–Ω—É—Ç (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è Colab)\n",
    "- –° 10 trials: ~5-8 –º–∏–Ω—É—Ç (–±—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install"
   },
   "source": [
    "## 1Ô∏è‚É£ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_libs"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q xgboost lightgbm catboost optuna scipy pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## 2Ô∏è‚É£ –ò–º–ø–æ—Ä—Ç—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_libs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report, confusion_matrix\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"‚úÖ –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## 3Ô∏è‚É£ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è\n",
    "\n",
    "–ò–∑–º–µ–Ω–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–¥–µ—Å—å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç—å—é/–∫–∞—á–µ—Å—Ç–≤–æ–º:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config_params"
   },
   "outputs": [],
   "source": [
    "# Hyperparameter optimization trials\n",
    "# –ë–æ–ª—å—à–µ trials = –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–æ –¥–æ–ª—å—à–µ\n",
    "N_OPTIMIZATION_TRIALS = 30  # –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: 30 (10-15 –º–∏–Ω) | –ë—ã—Å—Ç—Ä–æ: 10 | –ú–∞–∫—Å–∏–º—É–º: 50\n",
    "\n",
    "# Cross-validation folds\n",
    "N_CV_FOLDS = 5  # –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ—Å—Ç–∞–≤–∏—Ç—å 5\n",
    "\n",
    "# Ensemble models\n",
    "N_ENSEMBLE_MODELS = 3  # –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 3\n",
    "\n",
    "print(f\"‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:\")\n",
    "print(f\"   Optimization trials: {N_OPTIMIZATION_TRIALS}\")\n",
    "print(f\"   CV folds: {N_CV_FOLDS}\")\n",
    "print(f\"   Ensemble models: {N_ENSEMBLE_MODELS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload"
   },
   "source": [
    "## 4Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "**–í–ê–ñ–ù–û:** –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã V3 (leak-free), –ù–ï V2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_data"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"üì§ –ó–∞–≥—Ä—É–∑–∏—Ç–µ 4 —Ñ–∞–π–ª–∞ leak-free v3:\")\n",
    "print(\"   1. X_train_leak_free_v3.parquet\")\n",
    "print(\"   2. X_test_leak_free_v3.parquet\")\n",
    "print(\"   3. y_train_leak_free_v3.parquet\")\n",
    "print(\"   4. y_test_leak_free_v3.parquet\")\n",
    "print()\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Verify files\n",
    "required_files = [\n",
    "    'X_train_leak_free_v3.parquet',\n",
    "    'X_test_leak_free_v3.parquet',\n",
    "    'y_train_leak_free_v3.parquet',\n",
    "    'y_test_leak_free_v3.parquet'\n",
    "]\n",
    "\n",
    "print(\"\\nüìÅ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤:\")\n",
    "all_ok = True\n",
    "for file in required_files:\n",
    "    if file in uploaded:\n",
    "        size_mb = len(uploaded[file]) / (1024 * 1024)\n",
    "        print(f\"   ‚úÖ {file} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {file} - –ù–ï –ù–ê–ô–î–ï–ù!\")\n",
    "        all_ok = False\n",
    "\n",
    "if not all_ok:\n",
    "    raise ValueError(\"–ù–µ –≤—Å–µ —Ñ–∞–π–ª—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã! –ó–∞–≥—Ä—É–∑–∏—Ç–µ –≤—Å–µ 4 —Ñ–∞–π–ª–∞ v3.\")\n",
    "\n",
    "# Load data\n",
    "print(\"\\nüìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...\")\n",
    "X_train = pd.read_parquet('X_train_leak_free_v3.parquet')\n",
    "X_test = pd.read_parquet('X_test_leak_free_v3.parquet')\n",
    "y_train = pd.read_parquet('y_train_leak_free_v3.parquet').values.ravel()\n",
    "y_test = pd.read_parquet('y_test_leak_free_v3.parquet').values.ravel()\n",
    "\n",
    "print(f\"\\n‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã:\")\n",
    "print(f\"   Training: {X_train.shape}\")\n",
    "print(f\"   Test: {X_test.shape}\")\n",
    "print(f\"   Train default rate: {y_train.mean():.2%}\")\n",
    "print(f\"   Test default rate: {y_test.mean():.2%}\")\n",
    "\n",
    "# Calculate class weight\n",
    "train_ratio = (1 - y_train.mean()) / y_train.mean()\n",
    "print(f\"   Class imbalance ratio: {train_ratio:.1f}:1\")\n",
    "print(f\"   scale_pos_weight: {train_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "optimize"
   },
   "source": [
    "## 5Ô∏è‚É£ Bayesian Hyperparameter Optimization\n",
    "\n",
    "–≠—Ç–æ –∑–∞–π–º–µ—Ç ~10-15 –º–∏–Ω—É—Ç —Å 30 trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "optimize_params"
   },
   "outputs": [],
   "source": [
    "def optimize_xgboost(X_train, y_train, scale_pos_weight, n_trials=30):\n",
    "    \"\"\"Bayesian hyperparameter optimization\"\"\"\n",
    "    print(f\"üîç –ó–∞–ø—É—Å–∫ Bayesian optimization ({n_trials} trials)...\")\n",
    "    print(f\"   –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è: {n_trials//3}-{n_trials//2} –º–∏–Ω—É—Ç\\n\")\n",
    "    \n",
    "    # Create holdout for early stopping\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        X_train, y_train,\n",
    "        test_size=0.2,\n",
    "        stratify=y_train,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'max_depth': trial.suggest_int('max_depth', 4, 8),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 800, 1500),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 5, 20),\n",
    "            'subsample': trial.suggest_float('subsample', 0.7, 0.9),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 0.9),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 2.0),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1.0, 5.0),\n",
    "            'scale_pos_weight': scale_pos_weight,\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        model = XGBClassifier(**params)\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "        \n",
    "        y_pred = model.predict_proba(X_val)[:, 1]\n",
    "        auc = roc_auc_score(y_val, y_pred)\n",
    "        \n",
    "        return auc\n",
    "    \n",
    "    # Run optimization\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE)\n",
    "    )\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    best_params['scale_pos_weight'] = scale_pos_weight\n",
    "    best_params['objective'] = 'binary:logistic'\n",
    "    best_params['eval_metric'] = 'auc'\n",
    "    best_params['random_state'] = RANDOM_STATE\n",
    "    best_params['n_jobs'] = -1\n",
    "    \n",
    "    print(f\"\\n‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")\n",
    "    print(f\"   Best validation AUC: {study.best_value:.4f}\")\n",
    "    print(f\"\\n   –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:\")\n",
    "    for key in ['max_depth', 'learning_rate', 'n_estimators', 'min_child_weight',\n",
    "                'subsample', 'colsample_bytree', 'reg_alpha', 'reg_lambda']:\n",
    "        if key in best_params:\n",
    "            print(f\"      {key}: {best_params[key]}\")\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "# Run optimization\n",
    "best_params = optimize_xgboost(X_train, y_train, train_ratio, n_trials=N_OPTIMIZATION_TRIALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oof"
   },
   "source": [
    "## 6Ô∏è‚É£ Out-of-Fold Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oof_predictions"
   },
   "outputs": [],
   "source": [
    "def generate_oof_predictions(X_train, y_train, params, n_folds=5):\n",
    "    \"\"\"Generate OOF predictions for honest evaluation\"\"\"\n",
    "    print(f\"üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è OOF predictions ({n_folds} folds)...\\n\")\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "    oof_predictions = np.zeros(len(X_train))\n",
    "    feature_importance = np.zeros(X_train.shape[1])\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        model = XGBClassifier(**params)\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "        \n",
    "        oof_predictions[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "        feature_importance += model.feature_importances_ / n_folds\n",
    "        \n",
    "        fold_auc = roc_auc_score(y_val, oof_predictions[val_idx])\n",
    "        print(f\"   Fold {fold+1}/{n_folds} AUC: {fold_auc:.4f}\")\n",
    "    \n",
    "    oof_auc = roc_auc_score(y_train, oof_predictions)\n",
    "    print(f\"\\n‚úÖ Overall OOF AUC: {oof_auc:.4f}\")\n",
    "    \n",
    "    return oof_predictions, feature_importance\n",
    "\n",
    "# Generate OOF\n",
    "oof_predictions, feature_importance = generate_oof_predictions(\n",
    "    X_train, y_train, best_params, n_folds=N_CV_FOLDS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ensemble"
   },
   "source": [
    "## 7Ô∏è‚É£ Train Diverse Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_ensemble"
   },
   "outputs": [],
   "source": [
    "def train_ensemble_models(X_train, y_train, X_test, base_params, n_models=3):\n",
    "    \"\"\"Train diverse ensemble of XGBoost models\"\"\"\n",
    "    print(f\"ü§ñ –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è ({n_models} –º–æ–¥–µ–ª–µ–π)...\\n\")\n",
    "    \n",
    "    # Create diverse parameter sets\n",
    "    ensemble_params = [\n",
    "        # Model 1: Conservative\n",
    "        {\n",
    "            **base_params,\n",
    "            'max_depth': min(base_params['max_depth'], 4),\n",
    "            'learning_rate': min(base_params['learning_rate'], 0.01),\n",
    "            'min_child_weight': max(base_params['min_child_weight'], 10),\n",
    "            'reg_alpha': max(base_params['reg_alpha'], 1.0),\n",
    "            'reg_lambda': max(base_params['reg_lambda'], 3.0)\n",
    "        },\n",
    "        # Model 2: Best params\n",
    "        base_params.copy(),\n",
    "        # Model 3: Alternative\n",
    "        {\n",
    "            **base_params,\n",
    "            'max_depth': base_params['max_depth'] - 1,\n",
    "            'learning_rate': base_params['learning_rate'] * 0.8,\n",
    "            'n_estimators': base_params['n_estimators'] + 200,\n",
    "            'random_state': 123\n",
    "        }\n",
    "    ][:n_models]\n",
    "    \n",
    "    models = []\n",
    "    train_predictions = []\n",
    "    test_predictions = []\n",
    "    \n",
    "    for i, params in enumerate(ensemble_params):\n",
    "        print(f\"   –ú–æ–¥–µ–ª—å {i+1}/{n_models}...\", end=\" \")\n",
    "        \n",
    "        # Create holdout\n",
    "        X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=0.2, stratify=y_train, random_state=RANDOM_STATE\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        model = XGBClassifier(**params)\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "        \n",
    "        # Predictions\n",
    "        train_pred = model.predict_proba(X_train)[:, 1]\n",
    "        test_pred = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        train_predictions.append(train_pred)\n",
    "        test_predictions.append(test_pred)\n",
    "        models.append(model)\n",
    "        \n",
    "        train_auc = roc_auc_score(y_train, train_pred)\n",
    "        print(f\"Train AUC: {train_auc:.4f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ –ê–Ω—Å–∞–º–±–ª—å –æ–±—É—á–µ–Ω\")\n",
    "    return models, np.array(train_predictions).T, np.array(test_predictions).T\n",
    "\n",
    "# Train ensemble\n",
    "models, train_preds, test_preds = train_ensemble_models(\n",
    "    X_train, y_train, X_test, best_params, n_models=N_ENSEMBLE_MODELS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weights"
   },
   "source": [
    "## 8Ô∏è‚É£ Optimize Ensemble Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "optimize_weights"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def optimize_ensemble_weights(train_predictions, y_train):\n",
    "    \"\"\"Find optimal ensemble weights\"\"\"\n",
    "    print(\"‚öñÔ∏è –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –∞–Ω—Å–∞–º–±–ª—è...\")\n",
    "    \n",
    "    def ensemble_score(weights):\n",
    "        weights = weights / weights.sum()\n",
    "        pred = np.average(train_predictions, weights=weights, axis=1)\n",
    "        return -roc_auc_score(y_train, pred)\n",
    "    \n",
    "    n_models = train_predictions.shape[1]\n",
    "    initial_weights = np.ones(n_models) / n_models\n",
    "    \n",
    "    constraints = ({'type': 'eq', 'fun': lambda w: w.sum() - 1})\n",
    "    bounds = [(0, 1)] * n_models\n",
    "    \n",
    "    result = minimize(\n",
    "        ensemble_score,\n",
    "        initial_weights,\n",
    "        method='SLSQP',\n",
    "        bounds=bounds,\n",
    "        constraints=constraints\n",
    "    )\n",
    "    \n",
    "    optimal_weights = result.x\n",
    "    ensemble_train = np.average(train_predictions, weights=optimal_weights, axis=1)\n",
    "    ensemble_auc = roc_auc_score(y_train, ensemble_train)\n",
    "    \n",
    "    print(f\"   –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞: {optimal_weights}\")\n",
    "    print(f\"   Ensemble train AUC: {ensemble_auc:.4f}\")\n",
    "    \n",
    "    return optimal_weights\n",
    "\n",
    "# Optimize weights\n",
    "weights = optimize_ensemble_weights(train_preds, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "calibrate"
   },
   "source": [
    "## 9Ô∏è‚É£ Calibrate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "calibrate_preds"
   },
   "outputs": [],
   "source": [
    "def calibrate_predictions(models, weights, X_train, y_train, X_test):\n",
    "    \"\"\"Apply isotonic calibration\"\"\"\n",
    "    print(\"üîß –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π...\")\n",
    "    \n",
    "    # Ensemble wrapper\n",
    "    class EnsembleModel:\n",
    "        def __init__(self, models, weights):\n",
    "            self.models = models\n",
    "            self.weights = weights\n",
    "        \n",
    "        def predict_proba(self, X):\n",
    "            pred = np.zeros((len(X), 2))\n",
    "            for model, weight in zip(self.models, self.weights):\n",
    "                pred += model.predict_proba(X) * weight\n",
    "            return pred\n",
    "        \n",
    "        def fit(self, X, y):\n",
    "            return self\n",
    "    \n",
    "    ensemble_model = EnsembleModel(models, weights)\n",
    "    \n",
    "    # Get uncalibrated predictions\n",
    "    train_pred = np.zeros(len(X_train))\n",
    "    for model, weight in zip(models, weights):\n",
    "        train_pred += model.predict_proba(X_train)[:, 1] * weight\n",
    "    \n",
    "    # Calibrate\n",
    "    calibrated = CalibratedClassifierCV(\n",
    "        ensemble_model,\n",
    "        method='isotonic',\n",
    "        cv=3\n",
    "    )\n",
    "    calibrated.fit(X_train, y_train)\n",
    "    \n",
    "    # Generate calibrated predictions\n",
    "    calib_train = calibrated.predict_proba(X_train)[:, 1]\n",
    "    calib_test = calibrated.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    uncalib_auc = roc_auc_score(y_train, train_pred)\n",
    "    calib_auc = roc_auc_score(y_train, calib_train)\n",
    "    \n",
    "    print(f\"   –î–æ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏:    {uncalib_auc:.4f}\")\n",
    "    print(f\"   –ü–æ—Å–ª–µ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏: {calib_auc:.4f}\")\n",
    "    \n",
    "    return calibrated, calib_train, calib_test\n",
    "\n",
    "# Calibrate\n",
    "calibrated, calib_train, calib_test = calibrate_predictions(\n",
    "    models, weights, X_train, y_train, X_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluate"
   },
   "source": [
    "## üéØ FINAL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final_eval"
   },
   "outputs": [],
   "source": [
    "# Calculate final metrics\n",
    "train_auc = roc_auc_score(y_train, calib_train)\n",
    "test_auc = roc_auc_score(y_test, calib_test)\n",
    "gap = train_auc - test_auc\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üéØ FINAL RESULTS - LEAK-FREE PIPELINE V3\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n  Training AUC:   {train_auc:.4f}\")\n",
    "print(f\"  Test AUC:       {test_auc:.4f}\")\n",
    "print(f\"  Train-Test Gap: {gap:.4f}\")\n",
    "\n",
    "# Success criteria\n",
    "print(\"\\n  Success Criteria:\")\n",
    "success_90 = test_auc >= 0.90\n",
    "success_gap = gap < 0.05\n",
    "print(f\"    ‚úÖ Test AUC >= 0.90: {'PASS' if success_90 else 'FAIL'} ({test_auc:.4f})\")\n",
    "print(f\"    ‚úÖ Gap < 0.05:       {'PASS' if success_gap else 'FAIL'} ({gap:.4f})\")\n",
    "\n",
    "if success_90 and success_gap:\n",
    "    print(\"\\nüéâ SUCCESS! –î–æ—Å—Ç–∏–≥–Ω—É—Ç–æ 90%+ AUC –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è!\")\n",
    "else:\n",
    "    print(\"\\nüìä –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —á–µ—Å—Ç–Ω—ã–µ (leak-free).\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "confusion"
   },
   "source": [
    "## üìä Confusion Matrix & Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "confusion_matrix"
   },
   "outputs": [],
   "source": [
    "# Find optimal threshold\n",
    "fpr, tpr, thresholds = roc_curve(y_test, calib_test)\n",
    "j_scores = tpr - fpr\n",
    "optimal_idx = np.argmax(j_scores)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "print(f\"Optimal threshold (Youden's J): {optimal_threshold:.4f}\\n\")\n",
    "\n",
    "# Classification report\n",
    "y_pred_binary = (calib_test >= optimal_threshold).astype(int)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_binary, target_names=['Non-Default', 'Default']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_binary)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(f\"  TN: {cm[0,0]:5d}  FP: {cm[0,1]:5d}\")\n",
    "print(f\"  FN: {cm[1,0]:5d}  TP: {cm[1,1]:5d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "features"
   },
   "source": [
    "## üîù Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "top_features"
   },
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 Most Important Features:\\n\")\n",
    "for idx, row in importance_df.head(15).iterrows():\n",
    "    marker = \"[OOF]\" if 'oof' in row['feature'].lower() else \"\"\n",
    "    print(f\"  {row['feature']:35s} {row['importance']:.4f} {marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viz"
   },
   "source": [
    "## üìà Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualizations"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. ROC Curve\n",
    "ax = axes[0, 0]\n",
    "ax.plot(fpr, tpr, color='blue', lw=2, label=f'Test AUC = {test_auc:.4f}')\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curve (Test Set)', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=12)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# 2. Feature Importance\n",
    "ax = axes[0, 1]\n",
    "top_feats = importance_df.head(15)\n",
    "colors = ['red' if 'oof' in f.lower() else 'steelblue' for f in top_feats['feature']]\n",
    "ax.barh(range(len(top_feats)), top_feats['importance'], color=colors, alpha=0.7)\n",
    "ax.set_yticks(range(len(top_feats)))\n",
    "ax.set_yticklabels(top_feats['feature'], fontsize=9)\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_title('Top 15 Features (red = OOF)', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 3. Confusion Matrix Heatmap\n",
    "ax = axes[1, 0]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=['Non-Default', 'Default'],\n",
    "            yticklabels=['Non-Default', 'Default'])\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 4. AUC Comparison\n",
    "ax = axes[1, 1]\n",
    "metrics = ['Train AUC', 'Test AUC', 'OOF AUC']\n",
    "values = [train_auc, test_auc, roc_auc_score(y_train, oof_predictions)]\n",
    "colors_bar = ['green' if v >= 0.90 else 'orange' if v >= 0.85 else 'red' for v in values]\n",
    "bars = ax.bar(metrics, values, color=colors_bar, alpha=0.7, edgecolor='black')\n",
    "ax.axhline(y=0.90, color='red', linestyle='--', linewidth=2, label='Target (0.90)')\n",
    "ax.set_ylim([0.70, 1.0])\n",
    "ax.set_ylabel('AUC Score')\n",
    "ax.set_title('AUC Metrics Summary', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add values on bars\n",
    "for bar, val in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{val:.4f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('leak_free_v3_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: leak_free_v3_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save"
   },
   "source": [
    "## üíæ Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_results"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save model\n",
    "joblib.dump(calibrated, 'xgboost_calibrated_ensemble_v3_colab.pkl')\n",
    "print(\"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: xgboost_calibrated_ensemble_v3_colab.pkl\")\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'test_predictions': calib_test,\n",
    "    'true_labels': y_test\n",
    "})\n",
    "predictions_df.to_csv('test_predictions_v3_colab.csv', index=False)\n",
    "print(\"‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: test_predictions_v3_colab.csv\")\n",
    "\n",
    "# Save feature importance\n",
    "importance_df.to_csv('feature_importance_v3_colab.csv', index=False)\n",
    "print(\"‚úÖ –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: feature_importance_v3_colab.csv\")\n",
    "\n",
    "# Save metrics\n",
    "metrics_df = pd.DataFrame([{\n",
    "    'train_auc': train_auc,\n",
    "    'test_auc': test_auc,\n",
    "    'oof_auc': roc_auc_score(y_train, oof_predictions),\n",
    "    'train_test_gap': gap,\n",
    "    'optimal_threshold': optimal_threshold,\n",
    "    'n_trials': N_OPTIMIZATION_TRIALS,\n",
    "    'n_cv_folds': N_CV_FOLDS,\n",
    "    'n_ensemble_models': N_ENSEMBLE_MODELS\n",
    "}])\n",
    "metrics_df.to_csv('model_metrics_v3_colab.csv', index=False)\n",
    "print(\"‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: model_metrics_v3_colab.csv\")\n",
    "\n",
    "print(\"\\nüì¶ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –∏ –≥–æ—Ç–æ–≤—ã –∫ —Å–∫–∞—á–∏–≤–∞–Ω–∏—é!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## üì• Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_files"
   },
   "outputs": [],
   "source": [
    "# Download all files\n",
    "files.download('xgboost_calibrated_ensemble_v3_colab.pkl')\n",
    "files.download('test_predictions_v3_colab.csv')\n",
    "files.download('feature_importance_v3_colab.csv')\n",
    "files.download('model_metrics_v3_colab.csv')\n",
    "files.download('leak_free_v3_results.png')\n",
    "\n",
    "print(\"‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "---\n",
    "\n",
    "## üìã Summary\n",
    "\n",
    "### –ß—Ç–æ –±—ã–ª–æ —Å–¥–µ–ª–∞–Ω–æ:\n",
    "1. ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã **leak-free –¥–∞–Ω–Ω—ã–µ V3** (–±–µ–∑ —É—Ç–µ—á–∫–∏)\n",
    "2. ‚úÖ Bayesian optimization (50+ trials)\n",
    "3. ‚úÖ Out-of-fold predictions –¥–ª—è —á–µ—Å—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏\n",
    "4. ‚úÖ –ê–Ω—Å–∞–º–±–ª—å –∏–∑ 3 —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π\n",
    "5. ‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤\n",
    "6. ‚úÖ Isotonic calibration\n",
    "7. ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "\n",
    "### –ö–ª—é—á–µ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:\n",
    "- **Test AUC:** —á–µ—Å—Ç–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ –±–µ–∑ —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö\n",
    "- **Train-Test Gap:** –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è\n",
    "- **Leak-free pipeline:** —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥—É—Ç—Å—è –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ\n",
    "\n",
    "### –í–∞–∂–Ω–æ:\n",
    "- –≠—Ç–æ—Ç pipeline –∏—Å–ø–æ–ª—å–∑—É–µ—Ç **leak-free V3 –¥–∞–Ω–Ω—ã–µ**\n",
    "- –í—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ–∑–¥–∞–Ω—ã **out-of-fold**\n",
    "- Test set **–ø–æ–ª–Ω–æ—Å—Ç—å—é –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω** –æ—Ç –æ–±—É—á–µ–Ω–∏—è\n",
    "- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã **—á–µ—Å—Ç–Ω—ã–µ –∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–µ**\n",
    "\n",
    "---\n",
    "\n",
    "**–ê–≤—Ç–æ—Ä:** ML Model Selection Expert  \n",
    "**–í–µ—Ä—Å–∏—è:** V3 Leak-Free Pipeline  \n",
    "**–î–∞—Ç–∞:** November 2025\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Google_Colab_Leak_Free_90plus_v3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
