{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîß LEAK-FREE DATA PREPARATION PIPELINE V3\n",
    "## –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏ –∫—Ä–µ–¥–∏—Ç–Ω–æ–≥–æ —Å–∫–æ—Ä–∏–Ω–≥–∞\n",
    "\n",
    "---\n",
    "\n",
    "### üìã –ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –Ω–æ—É—Ç–±—É–∫:\n",
    "1. ‚úÖ –ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑–æ–≤—ã–µ –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (`X_train_optimized.parquet`, `X_test_optimized.parquet`)\n",
    "2. ‚úÖ –°–æ–∑–¥–∞–µ—Ç OOF (Out-of-Fold) KNN –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "3. ‚úÖ –°–æ–∑–¥–∞–µ—Ç OOF Target Encoding –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö\n",
    "4. ‚úÖ –°–æ–∑–¥–∞–µ—Ç OOF WOE (Weight of Evidence) –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "5. ‚úÖ –°–æ–∑–¥–∞–µ—Ç Interaction –∏ Polynomial –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "6. ‚úÖ –°–æ—Ö—Ä–∞–Ω—è–µ—Ç leak-free –¥–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
    "\n",
    "### ‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:\n",
    "- ~5-8 –º–∏–Ω—É—Ç –Ω–∞ Google Colab\n",
    "\n",
    "### üì¶ –í—ã—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã:\n",
    "- `X_train_leak_free_v3.parquet` (71,999 √ó 89)\n",
    "- `X_test_leak_free_v3.parquet` (18,000 √ó 89)\n",
    "- `y_train_leak_free_v3.parquet`\n",
    "- `y_test_leak_free_v3.parquet`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q pandas numpy scikit-learn pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ –ò–º–ø–æ—Ä—Ç—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LEAK-FREE DATA PIPELINE V3\")\n",
    "print(\"–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å Out-of-Fold –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–µ–π\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "**–í–ê–ñ–ù–û:** –ó–∞–≥—Ä—É–∑–∏—Ç–µ 4 —Ñ–∞–π–ª–∞:\n",
    "- `X_train_optimized.parquet`\n",
    "- `X_test_optimized.parquet`\n",
    "- `y_train.parquet`\n",
    "- `y_test.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"üì§ –ó–∞–≥—Ä—É–∑–∏—Ç–µ 4 –±–∞–∑–æ–≤—ã—Ö —Ñ–∞–π–ª–∞:\")\n",
    "print(\"   1. X_train_optimized.parquet\")\n",
    "print(\"   2. X_test_optimized.parquet\")\n",
    "print(\"   3. y_train.parquet\")\n",
    "print(\"   4. y_test.parquet\")\n",
    "print()\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Verify files\n",
    "required_files = [\n",
    "    'X_train_optimized.parquet',\n",
    "    'X_test_optimized.parquet',\n",
    "    'y_train.parquet',\n",
    "    'y_test.parquet'\n",
    "]\n",
    "\n",
    "print(\"\\nüìÅ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤:\")\n",
    "all_ok = True\n",
    "for file in required_files:\n",
    "    if file in uploaded:\n",
    "        size_mb = len(uploaded[file]) / (1024 * 1024)\n",
    "        print(f\"   ‚úÖ {file} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {file} - –ù–ï –ù–ê–ô–î–ï–ù!\")\n",
    "        all_ok = False\n",
    "\n",
    "if not all_ok:\n",
    "    raise ValueError(\"–ù–µ –≤—Å–µ —Ñ–∞–π–ª—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[1/8] –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...\")\n",
    "\n",
    "X_train = pd.read_parquet('X_train_optimized.parquet')\n",
    "X_test = pd.read_parquet('X_test_optimized.parquet')\n",
    "y_train = pd.read_parquet('y_train.parquet')['default'].values\n",
    "y_test = pd.read_parquet('y_test.parquet')['default'].values\n",
    "\n",
    "print(f\"‚úì Training set: {X_train.shape}\")\n",
    "print(f\"‚úì Test set: {X_test.shape}\")\n",
    "print(f\"‚úì Default rate: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"‚úì Class distribution will remain UNCHANGED (no SMOTE)\")\n",
    "\n",
    "# Create copies for feature engineering\n",
    "X_train_clean = X_train.copy()\n",
    "X_test_clean = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ OOF KNN Meta-Features\n",
    "\n",
    "–°–æ–∑–¥–∞–Ω–∏–µ KNN-based –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Out-of-Fold –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏.\n",
    "\n",
    "**–ö–ª—é—á–µ–≤–æ–π –ø—Ä–∏–Ω—Ü–∏–ø:** –ö–∞–∂–¥—ã–π fold –æ–±—É—á–∞–µ—Ç—Å—è –¢–û–õ–¨–ö–û –Ω–∞ —Å–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_oof_knn_features(X, y, X_test, n_neighbors=50, n_splits=5):\n",
    "    \"\"\"\n",
    "    Create KNN meta-features using strict out-of-fold methodology.\n",
    "    \n",
    "    CRITICAL: KNN is trained separately for each fold, ONLY on that fold's\n",
    "    training data, preventing any leakage.\n",
    "    \"\"\"\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    oof_train = np.zeros(len(X))\n",
    "    oof_test_folds = []\n",
    "    \n",
    "    for fold_num, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"  Processing fold {fold_num + 1}/{n_splits}...\", end=\" \")\n",
    "        \n",
    "        # CRITICAL: Fit scaler ONLY on training fold\n",
    "        scaler = StandardScaler()\n",
    "        X_train_fold = scaler.fit_transform(X[numeric_cols].iloc[train_idx])\n",
    "        X_val_fold = scaler.transform(X[numeric_cols].iloc[val_idx])\n",
    "        X_test_fold = scaler.transform(X_test[numeric_cols])\n",
    "        \n",
    "        # CRITICAL: Train KNN ONLY on training fold\n",
    "        knn = KNeighborsClassifier(n_neighbors=n_neighbors, n_jobs=-1)\n",
    "        knn.fit(X_train_fold, y[train_idx])\n",
    "        \n",
    "        # Predict on validation fold (OOF predictions)\n",
    "        oof_train[val_idx] = knn.predict_proba(X_val_fold)[:, 1]\n",
    "        \n",
    "        # Predict on test set (will average across folds)\n",
    "        oof_test_folds.append(knn.predict_proba(X_test_fold)[:, 1])\n",
    "        print(\"‚úì\")\n",
    "    \n",
    "    # Average test predictions across all folds\n",
    "    oof_test = np.mean(oof_test_folds, axis=0)\n",
    "    \n",
    "    return oof_train, oof_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[2/8] Creating OOF KNN meta-features...\")\n",
    "print(\"Each fold's KNN is trained ONLY on that fold's training data\\n\")\n",
    "\n",
    "# Create KNN features for different K values\n",
    "for n_neighbors in [50, 100, 500]:\n",
    "    print(f\"\\n  Creating OOF KNN feature with K={n_neighbors}\")\n",
    "    oof_train, oof_test = create_oof_knn_features(\n",
    "        X_train, y_train, X_test, n_neighbors=n_neighbors\n",
    "    )\n",
    "    \n",
    "    X_train_clean[f'knn_oof_{n_neighbors}'] = oof_train\n",
    "    X_test_clean[f'knn_oof_{n_neighbors}'] = oof_test\n",
    "    \n",
    "    # Calculate correlation\n",
    "    corr = np.corrcoef(oof_train, y_train)[0, 1]\n",
    "    print(f\"  ‚úì knn_oof_{n_neighbors} created\")\n",
    "    print(f\"    Correlation with target: {corr:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ OOF KNN features created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ OOF Target Encoding\n",
    "\n",
    "–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å—Ä–µ–¥–Ω–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–µ—Ñ–æ–ª—Ç–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_oof_target_encoding(X, y, X_test, column, n_splits=5, smoothing=10):\n",
    "    \"\"\"\n",
    "    Create target encoding using strict out-of-fold methodology.\n",
    "    \n",
    "    CRITICAL: Encoding statistics calculated separately for each fold,\n",
    "    ONLY from that fold's training data.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    oof_train = np.zeros(len(X))\n",
    "    oof_test_folds = []\n",
    "    \n",
    "    for fold_num, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        # Calculate encoding ONLY on training fold\n",
    "        train_data = pd.DataFrame({\n",
    "            'feature': X[column].iloc[train_idx],\n",
    "            'target': y[train_idx]\n",
    "        })\n",
    "        \n",
    "        # Calculate smoothed mean (Bayesian-like smoothing)\n",
    "        encoding_dict = {}\n",
    "        global_mean = y[train_idx].mean()\n",
    "        \n",
    "        for value in train_data['feature'].unique():\n",
    "            mask = train_data['feature'] == value\n",
    "            n = mask.sum()\n",
    "            if n == 0:\n",
    "                encoding_dict[value] = global_mean\n",
    "            else:\n",
    "                # Smoothing to prevent overfitting on rare categories\n",
    "                category_mean = train_data.loc[mask, 'target'].mean()\n",
    "                encoding_dict[value] = (category_mean * n + global_mean * smoothing) / (n + smoothing)\n",
    "        \n",
    "        # Apply to validation fold\n",
    "        oof_train[val_idx] = X[column].iloc[val_idx].map(encoding_dict).fillna(global_mean)\n",
    "        \n",
    "        # Apply to test set\n",
    "        test_encoded = X_test[column].map(encoding_dict).fillna(global_mean)\n",
    "        oof_test_folds.append(test_encoded)\n",
    "    \n",
    "    # Average test encodings across folds\n",
    "    oof_test = np.mean(oof_test_folds, axis=0)\n",
    "    \n",
    "    return oof_train, oof_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[3/8] Creating OOF target encoding for categorical features...\")\n",
    "print(\"Each fold encodes using ONLY that fold's training statistics\\n\")\n",
    "\n",
    "# Apply OOF target encoding to categorical columns\n",
    "categorical_cols = ['state', 'marital_status', 'education', 'employment_type']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in X_train.columns:\n",
    "        print(f\"  Encoding {col}...\", end=\" \")\n",
    "        oof_train, oof_test = create_oof_target_encoding(\n",
    "            X_train, y_train, X_test, col\n",
    "        )\n",
    "        \n",
    "        X_train_clean[f'{col}_target_oof'] = oof_train\n",
    "        X_test_clean[f'{col}_target_oof'] = oof_test\n",
    "        \n",
    "        corr = np.corrcoef(oof_train, y_train)[0, 1]\n",
    "        print(f\"‚úì correlation = {corr:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ OOF Target Encoding complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ OOF WOE (Weight of Evidence)\n",
    "\n",
    "–°–æ–∑–¥–∞–Ω–∏–µ WOE –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_oof_woe(X, y, X_test, column, n_bins=10, n_splits=5):\n",
    "    \"\"\"\n",
    "    Create WOE binning using strict out-of-fold methodology.\n",
    "    \n",
    "    CRITICAL: Bins and WOE values calculated separately for each fold,\n",
    "    ONLY from that fold's training data.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    oof_train = np.zeros(len(X))\n",
    "    oof_test_folds = []\n",
    "    \n",
    "    for fold_num, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        # Create bins ONLY from training fold\n",
    "        train_values = X[column].iloc[train_idx]\n",
    "        \n",
    "        try:\n",
    "            # Use quantile bins\n",
    "            _, bins = pd.qcut(train_values, q=n_bins, duplicates='drop', retbins=True)\n",
    "            \n",
    "            # Bin the training data\n",
    "            train_binned = pd.cut(train_values, bins=bins, include_lowest=True)\n",
    "            \n",
    "            # Calculate WOE for each bin (ONLY from training fold)\n",
    "            woe_dict = {}\n",
    "            total_good = (y[train_idx] == 0).sum()\n",
    "            total_bad = y[train_idx].sum()\n",
    "            \n",
    "            for bin_label in train_binned.cat.categories:\n",
    "                mask = train_binned == bin_label\n",
    "                n_good = (y[train_idx][mask] == 0).sum()\n",
    "                n_bad = y[train_idx][mask].sum()\n",
    "                \n",
    "                # Add smoothing to avoid log(0)\n",
    "                n_good = max(n_good, 0.5)\n",
    "                n_bad = max(n_bad, 0.5)\n",
    "                \n",
    "                # Calculate WOE\n",
    "                pct_good = n_good / total_good\n",
    "                pct_bad = n_bad / total_bad\n",
    "                woe = np.log(pct_good / pct_bad)\n",
    "                woe_dict[bin_label] = woe\n",
    "            \n",
    "            # Apply to validation fold\n",
    "            val_binned = pd.cut(X[column].iloc[val_idx], bins=bins, include_lowest=True)\n",
    "            oof_train[val_idx] = val_binned.map(woe_dict).fillna(0)\n",
    "            \n",
    "            # Apply to test set\n",
    "            test_binned = pd.cut(X_test[column], bins=bins, include_lowest=True)\n",
    "            test_woe = test_binned.map(woe_dict).fillna(0)\n",
    "            oof_test_folds.append(test_woe)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # If binning fails, use 0s\n",
    "            oof_train[val_idx] = 0\n",
    "            oof_test_folds.append(np.zeros(len(X_test)))\n",
    "    \n",
    "    # Average test WOE across folds\n",
    "    oof_test = np.mean(oof_test_folds, axis=0)\n",
    "    \n",
    "    return oof_train, oof_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[4/8] Creating OOF WOE features...\")\n",
    "print(\"WOE bins calculated separately for each fold\\n\")\n",
    "\n",
    "# Apply OOF WOE to numeric features\n",
    "woe_features = ['debt_to_income_ratio', 'credit_utilization', 'credit_score', 'age']\n",
    "\n",
    "for feature in woe_features:\n",
    "    if feature in X_train.columns:\n",
    "        print(f\"  Creating OOF WOE for {feature}...\", end=\" \")\n",
    "        oof_train, oof_test = create_oof_woe(\n",
    "            X_train, y_train, X_test, feature\n",
    "        )\n",
    "        \n",
    "        X_train_clean[f'{feature}_woe_oof'] = oof_train\n",
    "        X_test_clean[f'{feature}_woe_oof'] = oof_test\n",
    "        \n",
    "        corr = np.corrcoef(oof_train, y_train)[0, 1]\n",
    "        print(f\"‚úì correlation = {corr:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ OOF WOE features created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Interaction Features\n",
    "\n",
    "–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è (–ù–ï–¢ —É—Ç–µ—á–∫–∏ - –ø—Ä–æ—Å—Ç–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[5/8] Creating interaction features...\")\n",
    "print(\"Simple multiplication - no leakage possible\\n\")\n",
    "\n",
    "# Debt burden interactions\n",
    "if 'debt_to_income_ratio' in X_train.columns and 'credit_utilization' in X_train.columns:\n",
    "    X_train_clean['debt_credit_interaction'] = X_train['debt_to_income_ratio'] * X_train['credit_utilization']\n",
    "    X_test_clean['debt_credit_interaction'] = X_test['debt_to_income_ratio'] * X_test['credit_utilization']\n",
    "    print(f\"  ‚úì debt_credit_interaction created\")\n",
    "\n",
    "# Income stability * debt burden\n",
    "if 'income_stability_score' in X_train.columns and 'debt_payment_burden' in X_train.columns:\n",
    "    X_train_clean['income_debt_interaction'] = X_train['income_stability_score'] * X_train['debt_payment_burden']\n",
    "    X_test_clean['income_debt_interaction'] = X_test['income_stability_score'] * X_test['debt_payment_burden']\n",
    "    print(f\"  ‚úì income_debt_interaction created\")\n",
    "\n",
    "# Age * credit score\n",
    "if 'age' in X_train.columns and 'credit_score' in X_train.columns:\n",
    "    X_train_clean['age_credit_interaction'] = X_train['age'] * X_train['credit_score'] / 100\n",
    "    X_test_clean['age_credit_interaction'] = X_test['age'] * X_test['credit_score'] / 100\n",
    "    print(f\"  ‚úì age_credit_interaction created\")\n",
    "\n",
    "# Employment length * income\n",
    "if 'employment_length' in X_train.columns and 'monthly_income' in X_train.columns:\n",
    "    X_train_clean['employment_income_stability'] = X_train['employment_length'] * X_train['monthly_income']\n",
    "    X_test_clean['employment_income_stability'] = X_test['employment_length'] * X_test['monthly_income']\n",
    "    print(f\"  ‚úì employment_income_stability created\")\n",
    "\n",
    "print(\"\\n‚úÖ Interaction features created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Polynomial Features\n",
    "\n",
    "–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ù–ï–¢ —É—Ç–µ—á–∫–∏ - –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[6/8] Creating polynomial features...\")\n",
    "print(\"Simple transformations - no leakage possible\\n\")\n",
    "\n",
    "top_features = ['credit_stress_score', 'debt_to_income_ratio', 'credit_utilization']\n",
    "\n",
    "for feature in top_features:\n",
    "    if feature in X_train.columns:\n",
    "        # Square\n",
    "        X_train_clean[f'{feature}_squared'] = X_train[feature] ** 2\n",
    "        X_test_clean[f'{feature}_squared'] = X_test[feature] ** 2\n",
    "        \n",
    "        # Cube root\n",
    "        X_train_clean[f'{feature}_cbrt'] = np.cbrt(X_train[feature])\n",
    "        X_test_clean[f'{feature}_cbrt'] = np.cbrt(X_test[feature])\n",
    "        \n",
    "        print(f\"  ‚úì {feature}_squared and {feature}_cbrt created\")\n",
    "\n",
    "print(\"\\n‚úÖ Polynomial features created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Validation & Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[7/8] Performing validation checks...\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "train_nulls = X_train_clean.isnull().sum().sum()\n",
    "test_nulls = X_test_clean.isnull().sum().sum()\n",
    "\n",
    "assert train_nulls == 0, f\"Training data has {train_nulls} missing values!\"\n",
    "assert test_nulls == 0, f\"Test data has {test_nulls} missing values!\"\n",
    "\n",
    "print(f\"‚úì No missing values in training data\")\n",
    "print(f\"‚úì No missing values in test data\")\n",
    "\n",
    "# Check feature alignment\n",
    "assert list(X_train_clean.columns) == list(X_test_clean.columns), \"Feature mismatch!\"\n",
    "print(f\"‚úì Feature alignment verified\")\n",
    "\n",
    "# Check class distribution\n",
    "print(f\"\\nClass distribution check:\")\n",
    "print(f\"  Training default rate: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"  Test default rate: {y_test.mean()*100:.2f}%\")\n",
    "print(f\"  ‚úì NO class balancing applied (correct approach)\")\n",
    "\n",
    "print(f\"\\nFEATURE SUMMARY:\")\n",
    "print(f\"  Original features: {X_train.shape[1]}\")\n",
    "print(f\"  Leak-free features: {X_train_clean.shape[1]}\")\n",
    "print(f\"  New OOF features: {X_train_clean.shape[1] - X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Save Leak-Free Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[8/8] Saving leak-free datasets...\\n\")\n",
    "\n",
    "# Save features\n",
    "X_train_clean.to_parquet('X_train_leak_free_v3.parquet', index=False)\n",
    "X_test_clean.to_parquet('X_test_leak_free_v3.parquet', index=False)\n",
    "\n",
    "# Save targets (original, no balancing)\n",
    "pd.DataFrame({'default': y_train}).to_parquet('y_train_leak_free_v3.parquet', index=False)\n",
    "pd.DataFrame({'default': y_test}).to_parquet('y_test_leak_free_v3.parquet', index=False)\n",
    "\n",
    "print(f\"‚úì Leak-free training features: X_train_leak_free_v3.parquet ({X_train_clean.shape})\")\n",
    "print(f\"‚úì Leak-free test features: X_test_leak_free_v3.parquet ({X_test_clean.shape})\")\n",
    "print(f\"‚úì Original training targets: y_train_leak_free_v3.parquet\")\n",
    "print(f\"‚úì Original test targets: y_test_leak_free_v3.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LEAK-FREE PIPELINE V3 COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nDATA LEAKAGE FIXES APPLIED:\")\n",
    "print(\"1. KNN meta-features: Using strict OOF methodology\")\n",
    "print(\"2. Target encoding: Using OOF with smoothing\")\n",
    "print(\"3. WOE binning: Calculated separately per fold\")\n",
    "print(\"4. StandardScaler: Fitted separately per fold\")\n",
    "print(\"5. SMOTE: NOT USED - will handle imbalance in model\")\n",
    "\n",
    "print(\"\\nCLASS DISTRIBUTION:\")\n",
    "print(f\"  Training: {(y_train == 1).sum():,} defaults / {len(y_train):,} total = {y_train.mean()*100:.2f}%\")\n",
    "print(f\"  Test: {(y_test == 1).sum():,} defaults / {len(y_test):,} total = {y_test.mean()*100:.2f}%\")\n",
    "\n",
    "print(\"\\nNEXT STEPS:\")\n",
    "print(\"  1. Download all 4 parquet files\")\n",
    "print(\"  2. Upload to Google_Colab_Leak_Free_90plus_v3.ipynb\")\n",
    "print(\"  3. Train XGBoost model\")\n",
    "print(\"  4. Expect realistic AUC (honest metrics)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Top 15 Features by Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop 15 features by correlation (LEAK-FREE):\\n\")\n",
    "correlations = {}\n",
    "for col in X_train_clean.columns:\n",
    "    try:\n",
    "        correlations[col] = abs(np.corrcoef(X_train_clean[col], y_train)[0, 1])\n",
    "    except:\n",
    "        correlations[col] = 0.0\n",
    "\n",
    "top_15 = sorted(correlations.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "for i, (feature, corr) in enumerate(top_15, 1):\n",
    "    new_marker = \" [OOF]\" if 'oof' in feature.lower() else \"\"\n",
    "    print(f\"{i:2d}. {feature:40s} {corr:.4f}{new_marker}\")\n",
    "\n",
    "print(\"\\n‚úÖ ALL FEATURES CREATED WITH OUT-OF-FOLD METHODOLOGY\")\n",
    "print(\"‚úÖ NO DATA LEAKAGE POSSIBLE\")\n",
    "print(\"‚úÖ READY FOR HONEST MODEL EVALUATION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Download Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all files\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤...\\n\")\n",
    "\n",
    "files.download('X_train_leak_free_v3.parquet')\n",
    "files.download('X_test_leak_free_v3.parquet')\n",
    "files.download('y_train_leak_free_v3.parquet')\n",
    "files.download('y_test_leak_free_v3.parquet')\n",
    "\n",
    "print(\"\\n‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã!\")\n",
    "print(\"\\nüéØ –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç—Ç–∏ —Ñ–∞–π–ª—ã –≤ Google_Colab_Leak_Free_90plus_v3.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã –ò—Ç–æ–≥–æ–≤–∞—è —Å–≤–æ–¥–∫–∞\n",
    "\n",
    "### –ß—Ç–æ –±—ã–ª–æ —Å–¥–µ–ª–∞–Ω–æ:\n",
    "1. ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –±–∞–∑–æ–≤—ã–µ –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "2. ‚úÖ –°–æ–∑–¥–∞–Ω—ã OOF KNN –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏ (K=50, 100, 500)\n",
    "3. ‚úÖ –°–æ–∑–¥–∞–Ω—ã OOF Target Encoding –¥–ª—è 4 –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö\n",
    "4. ‚úÖ –°–æ–∑–¥–∞–Ω—ã OOF WOE –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è 4 —á–∏—Å–ª–æ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö\n",
    "5. ‚úÖ –°–æ–∑–¥–∞–Ω—ã 4 Interaction –ø—Ä–∏–∑–Ω–∞–∫–∞\n",
    "6. ‚úÖ –°–æ–∑–¥–∞–Ω—ã 6 Polynomial –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "7. ‚úÖ –í—ã–ø–æ–ª–Ω–µ–Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—è (–Ω–µ—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤, –Ω–µ—Ç —É—Ç–µ—á–∫–∏)\n",
    "8. ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã leak-free –¥–∞—Ç–∞—Å–µ—Ç—ã\n",
    "\n",
    "### –ö–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:\n",
    "- **–ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è**: –°—Ç—Ä–æ–≥–∏–π Out-of-Fold (5 folds)\n",
    "- **–£—Ç–µ—á–∫–∞ –¥–∞–Ω–Ω—ã—Ö**: –û–¢–°–£–¢–°–¢–í–£–ï–¢ ‚úÖ\n",
    "- **–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞**: –ù–ï –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)\n",
    "- **–§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã**: Train (71,999 √ó 89), Test (18,000 √ó 89)\n",
    "\n",
    "### –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\n",
    "1. –°–∫–∞—á–∞–π—Ç–µ 4 parquet —Ñ–∞–π–ª–∞\n",
    "2. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏—Ö –≤ `Google_Colab_Leak_Free_90plus_v3.ipynb`\n",
    "3. –û–±—É—á–∏—Ç–µ XGBoost –º–æ–¥–µ–ª—å\n",
    "4. –ü–æ–ª—É—á–∏—Ç–µ —á–µ—Å—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ AUC\n",
    "\n",
    "---\n",
    "\n",
    "**–ê–≤—Ç–æ—Ä:** Data Preparation Specialist  \n",
    "**–í–µ—Ä—Å–∏—è:** V3 Leak-Free Pipeline  \n",
    "**–î–∞—Ç–∞:** –ù–æ—è–±—Ä—å 2025\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
