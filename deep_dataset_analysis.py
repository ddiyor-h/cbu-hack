"""
–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è credit default prediction
–¶–µ–ª—å: –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –ø–æ–ª–Ω—É—é –∫–∞—Ä—Ç–∏–Ω—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ml-data-preparation-specialist –∏ research-specialist
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import ks_2samp
import warnings
warnings.filterwarnings('ignore')

print("="*100)
print("–ì–õ–£–ë–û–ö–ò–ô –ê–ù–ê–õ–ò–ó –î–ê–¢–ê–°–ï–¢–ê - CREDIT DEFAULT PREDICTION")
print("="*100)

# ============================================================================
# 1. –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•
# ============================================================================

print("\n[1/10] –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

X_train = pd.read_parquet('/home/dr/cbu/X_train.parquet')
y_train = pd.read_parquet('/home/dr/cbu/y_train.parquet')
X_test = pd.read_parquet('/home/dr/cbu/X_test.parquet')
y_test = pd.read_parquet('/home/dr/cbu/y_test.parquet')

print(f"‚úÖ Train: {X_train.shape}, Test: {X_test.shape}")
print(f"‚úÖ Target: {y_train.shape}, {y_test.shape}")

# –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
df_train = X_train.copy()
df_train['default'] = y_train['default'].values

# ============================================================================
# 2. –ë–ê–ó–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê
# ============================================================================

print("\n" + "="*100)
print("[2/10] –ë–ê–ó–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ê–¢–ê–°–ï–¢–ê")
print("="*100)

print(f"\nüìä –†–ê–ó–ú–ï–†–´:")
print(f"   ‚Ä¢ –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(df_train):,} –∑–∞–ø–∏—Å–µ–π")
print(f"   ‚Ä¢ –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞:      {len(X_test):,} –∑–∞–ø–∏—Å–µ–π")
print(f"   ‚Ä¢ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π:         {len(df_train) + len(X_test):,}")
print(f"   ‚Ä¢ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤:             {X_train.shape[1]}")

# –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
default_rate_train = y_train['default'].mean()
default_rate_test = y_test['default'].mean()
imbalance_ratio = (1 - default_rate_train) / default_rate_train

print(f"\n‚öñÔ∏è  –ë–ê–õ–ê–ù–° –ö–õ–ê–°–°–û–í:")
print(f"   ‚Ä¢ Train default rate:    {default_rate_train:.2%} ({y_train['default'].sum():,} –¥–µ—Ñ–æ–ª—Ç–æ–≤)")
print(f"   ‚Ä¢ Test default rate:     {default_rate_test:.2%} ({y_test['default'].sum():,} –¥–µ—Ñ–æ–ª—Ç–æ–≤)")
print(f"   ‚Ä¢ Imbalance ratio:       1:{imbalance_ratio:.1f}")
print(f"   ‚Ä¢ Scale_pos_weight:      {imbalance_ratio:.1f}")

# –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = X_train.select_dtypes(exclude=[np.number]).columns.tolist()

print(f"\nüìã –¢–ò–ü–´ –ü–†–ò–ó–ù–ê–ö–û–í:")
print(f"   ‚Ä¢ –ß–∏—Å–ª–æ–≤—ã—Ö:      {len(numeric_features)}")
print(f"   ‚Ä¢ –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö: {len(categorical_features)}")

# ============================================================================
# 3. –ü–†–û–ü–£–©–ï–ù–ù–´–ï –ó–ù–ê–ß–ï–ù–ò–Ø
# ============================================================================

print("\n" + "="*100)
print("[3/10] –ê–ù–ê–õ–ò–ó –ü–†–û–ü–£–©–ï–ù–ù–´–• –ó–ù–ê–ß–ï–ù–ò–ô")
print("="*100)

missing_train = X_train.isnull().sum()
missing_test = X_test.isnull().sum()

features_with_missing = missing_train[missing_train > 0].sort_values(ascending=False)

if len(features_with_missing) > 0:
    print(f"\n‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ {len(features_with_missing)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏:")
    for feat, count in features_with_missing.head(10).items():
        pct = (count / len(X_train)) * 100
        print(f"   ‚Ä¢ {feat:50s}: {count:6,} ({pct:5.2f}%)")
else:
    print("\n‚úÖ –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ!")

# ============================================================================
# 4. –ê–ù–ê–õ–ò–ó –ß–ò–°–õ–û–í–´–• –ü–†–ò–ó–ù–ê–ö–û–í
# ============================================================================

print("\n" + "="*100)
print("[4/10] –ê–ù–ê–õ–ò–ó –ß–ò–°–õ–û–í–´–• –ü–†–ò–ó–ù–ê–ö–û–í")
print("="*100)

numeric_stats = X_train[numeric_features].describe().T
numeric_stats['skewness'] = X_train[numeric_features].skew()
numeric_stats['kurtosis'] = X_train[numeric_features].kurtosis()
numeric_stats['zeros_pct'] = (X_train[numeric_features] == 0).sum() / len(X_train) * 100

print(f"\nüìä –í—Å–µ–≥–æ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(numeric_features)}")

# –ò—â–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –≤—ã–±—Ä–æ—Å–∞–º–∏ (high skewness)
high_skew = numeric_stats[abs(numeric_stats['skewness']) > 3].sort_values('skewness', key=abs, ascending=False)

if len(high_skew) > 0:
    print(f"\n‚ö†Ô∏è  –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å –≤—ã—Å–æ–∫–æ–π –∞—Å–∏–º–º–µ—Ç—Ä–∏–µ–π (|skew| > 3): {len(high_skew)}")
    for feat in high_skew.head(10).index:
        print(f"   ‚Ä¢ {feat:50s}: skew={numeric_stats.loc[feat, 'skewness']:8.2f}")

# –ò—â–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ/–ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
low_variance = numeric_stats[numeric_stats['std'] < 0.01]
if len(low_variance) > 0:
    print(f"\n‚ö†Ô∏è  –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å –æ—á–µ–Ω—å –Ω–∏–∑–∫–æ–π –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å—é (std < 0.01): {len(low_variance)}")
    for feat in low_variance.index[:10]:
        print(f"   ‚Ä¢ {feat:50s}: std={numeric_stats.loc[feat, 'std']:.6f}")

# –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –Ω—É–ª–µ–π
high_zeros = numeric_stats[numeric_stats['zeros_pct'] > 50].sort_values('zeros_pct', ascending=False)
if len(high_zeros) > 0:
    print(f"\n‚ö†Ô∏è  –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å >50% –Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {len(high_zeros)}")
    for feat in high_zeros.head(10).index:
        print(f"   ‚Ä¢ {feat:50s}: {numeric_stats.loc[feat, 'zeros_pct']:.1f}% –Ω—É–ª–µ–π")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
numeric_stats.to_csv('/home/dr/cbu/numeric_features_statistics.csv')
print(f"\nüíæ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: numeric_features_statistics.csv")

# ============================================================================
# 5. –ê–ù–ê–õ–ò–ó –ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í
# ============================================================================

print("\n" + "="*100)
print("[5/10] –ê–ù–ê–õ–ò–ó –ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í")
print("="*100)

if len(categorical_features) > 0:
    print(f"\nüìä –í—Å–µ–≥–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(categorical_features)}")

    cat_stats = []
    for feat in categorical_features:
        n_unique = X_train[feat].nunique()
        most_common = X_train[feat].mode()[0] if len(X_train[feat].mode()) > 0 else None
        most_common_pct = (X_train[feat] == most_common).sum() / len(X_train) * 100 if most_common else 0

        cat_stats.append({
            'feature': feat,
            'unique_values': n_unique,
            'most_common': most_common,
            'most_common_pct': most_common_pct
        })

    cat_df = pd.DataFrame(cat_stats).sort_values('unique_values', ascending=False)

    print("\nüìã –¢–æ–ø-10 –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π:")
    for idx, row in cat_df.head(10).iterrows():
        print(f"   ‚Ä¢ {row['feature']:50s}: {row['unique_values']:6,} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö")

    # –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å –≤—ã—Å–æ–∫–æ–π –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é (>100 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö)
    high_cardinality = cat_df[cat_df['unique_values'] > 100]
    if len(high_cardinality) > 0:
        print(f"\n‚ö†Ô∏è  –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å –≤—ã—Å–æ–∫–æ–π –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é (>100): {len(high_cardinality)}")

    cat_df.to_csv('/home/dr/cbu/categorical_features_statistics.csv', index=False)
    print(f"\nüíæ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: categorical_features_statistics.csv")
else:
    print("\nüìã –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ—Ç (–≤—Å–µ one-hot encoded)")

# ============================================================================
# 6. –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í –ü–û –ö–õ–ê–°–°–ê–ú
# ============================================================================

print("\n" + "="*100)
print("[6/10] –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í –ü–û –ö–õ–ê–°–°–ê–ú")
print("="*100)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
target_corr = pd.read_csv('/home/dr/cbu/target_correlations.csv')
top_features = target_corr.head(20)['feature'].tolist()

print(f"\nüîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ø-20 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å default...")

class_separation = []

for feat in top_features[:10]:  # –ë–µ—Ä–µ–º —Ç–æ–ø-10 –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    if feat not in X_train.columns:
        continue

    default_vals = df_train[df_train['default'] == 1][feat]
    no_default_vals = df_train[df_train['default'] == 0][feat]

    # KS-—Ç–µ—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç–∏
    ks_stat, ks_pval = ks_2samp(default_vals.dropna(), no_default_vals.dropna())

    # –†–∞–∑–Ω–∏—Ü–∞ –º–µ–¥–∏–∞–Ω
    median_diff = default_vals.median() - no_default_vals.median()
    median_diff_pct = (median_diff / no_default_vals.median() * 100) if no_default_vals.median() != 0 else 0

    class_separation.append({
        'feature': feat,
        'ks_statistic': ks_stat,
        'ks_pvalue': ks_pval,
        'median_default': default_vals.median(),
        'median_no_default': no_default_vals.median(),
        'median_diff_pct': median_diff_pct
    })

sep_df = pd.DataFrame(class_separation).sort_values('ks_statistic', ascending=False)

print("\nüìä –†–ê–ó–î–ï–õ–ò–ú–û–°–¢–¨ –ö–õ–ê–°–°–û–í (Kolmogorov-Smirnov):")
print("="*100)
for idx, row in sep_df.iterrows():
    print(f"{row['feature']:50s}: KS={row['ks_statistic']:.4f}, Median Œî={row['median_diff_pct']:+7.1f}%")

sep_df.to_csv('/home/dr/cbu/class_separation_analysis.csv', index=False)
print(f"\nüíæ –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç–∏: class_separation_analysis.csv")

# ============================================================================
# 7. –ö–û–†–†–ï–õ–Ø–¶–ò–û–ù–ù–´–ô –ê–ù–ê–õ–ò–ó - –ò–ù–¢–ï–†–ê–ö–¶–ò–ò
# ============================================================================

print("\n" + "="*100)
print("[7/10] –ê–ù–ê–õ–ò–ó –ü–û–¢–ï–ù–¶–ò–ê–õ–¨–ù–´–• –ò–ù–¢–ï–†–ê–ö–¶–ò–ô")
print("="*100)

# –ë–µ—Ä–µ–º —Ç–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ç–µ—Ä–∞–∫—Ü–∏–π
top_10_features = target_corr.head(10)['feature'].tolist()
top_10_features = [f for f in top_10_features if f in X_train.columns]

print(f"\nüîç –ü–æ–∏—Å–∫ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–∞–∫—Ü–∏–π —Å—Ä–µ–¥–∏ —Ç–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

interaction_candidates = []

for i, feat1 in enumerate(top_10_features):
    for feat2 in top_10_features[i+1:]:
        # –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ—Ä–∞–∫—Ü–∏—é
        interaction = df_train[feat1] * df_train[feat2]

        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –∏–Ω—Ç–µ—Ä–∞–∫—Ü–∏–∏ —Å —Ç–∞—Ä–≥–µ—Ç–æ–º
        corr_with_target = interaction.corr(df_train['default'])

        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        corr_with_feat1 = interaction.corr(df_train[feat1])
        corr_with_feat2 = interaction.corr(df_train[feat2])

        # –ò–Ω—Ç–µ—Ä–∞–∫—Ü–∏—è –∏–Ω—Ç–µ—Ä–µ—Å–Ω–∞, –µ—Å–ª–∏ –æ–Ω–∞ —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç —Å —Ç–∞—Ä–≥–µ—Ç–æ–º
        # –Ω–æ –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –¥—É–±–ª–∏—Ä—É–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if abs(corr_with_target) > 0.1 and abs(corr_with_feat1) < 0.95 and abs(corr_with_feat2) < 0.95:
            interaction_candidates.append({
                'feature1': feat1,
                'feature2': feat2,
                'interaction_corr_target': corr_with_target,
                'corr_feat1': corr_with_feat1,
                'corr_feat2': corr_with_feat2,
                'interaction_name': f'{feat1}_x_{feat2}'
            })

if len(interaction_candidates) > 0:
    int_df = pd.DataFrame(interaction_candidates).sort_values('interaction_corr_target',
                                                               key=abs, ascending=False)

    print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(int_df)} –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–∞–∫—Ü–∏–π:")
    for idx, row in int_df.head(15).iterrows():
        print(f"   ‚Ä¢ {row['feature1']:30s} √ó {row['feature2']:30s}: corr={row['interaction_corr_target']:+.4f}")

    int_df.to_csv('/home/dr/cbu/interaction_candidates.csv', index=False)
    print(f"\nüíæ –ö–∞–Ω–¥–∏–¥–∞—Ç—ã –Ω–∞ –∏–Ω—Ç–µ—Ä–∞–∫—Ü–∏–∏: interaction_candidates.csv")
else:
    print("\n‚ö†Ô∏è  –ü–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–∞–∫—Ü–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

# ============================================================================
# 8. –í–†–ï–ú–ï–ù–ù–û–ô –ê–ù–ê–õ–ò–ó
# ============================================================================

print("\n" + "="*100)
print("[8/10] –í–†–ï–ú–ï–ù–ù–û–ô –ê–ù–ê–õ–ò–ó (ACCOUNT_OPEN_YEAR)")
print("="*100)

if 'account_open_year' in X_train.columns:
    print("\nüìÖ –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ –≥–æ–¥–∞–º...")

    temporal_stats = df_train.groupby('account_open_year').agg({
        'default': ['count', 'sum', 'mean']
    }).round(4)

    temporal_stats.columns = ['count', 'defaults', 'default_rate']
    temporal_stats['default_rate_pct'] = temporal_stats['default_rate'] * 100

    print("\nüìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –ì–û–î–ê–ú:")
    print(temporal_stats.to_string())

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç—Ä–µ–Ω–¥ –¥–µ—Ñ–æ–ª—Ç–Ω–æ—Å—Ç–∏
    years = temporal_stats.index.values
    default_rates = temporal_stats['default_rate'].values

    from scipy.stats import pearsonr
    corr_year_default, pval = pearsonr(years, default_rates)

    print(f"\nüìà –í–†–ï–ú–ï–ù–ù–û–ô –¢–†–ï–ù–î:")
    print(f"   ‚Ä¢ –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –≥–æ–¥-–¥–µ—Ñ–æ–ª—Ç: {corr_year_default:+.4f} (p={pval:.4f})")

    if abs(corr_year_default) > 0.3:
        trend = "–†–ê–°–¢–ï–¢" if corr_year_default > 0 else "–ü–ê–î–ê–ï–¢"
        print(f"   ‚Ä¢ ‚ö†Ô∏è  –î–µ—Ñ–æ–ª—Ç–Ω–æ—Å—Ç—å {trend} —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º!")
    else:
        print(f"   ‚Ä¢ ‚úÖ –î–µ—Ñ–æ–ª—Ç–Ω–æ—Å—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω–∞ –≤–æ –≤—Ä–µ–º–µ–Ω–∏")

    temporal_stats.to_csv('/home/dr/cbu/temporal_analysis.csv')
    print(f"\nüíæ –í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑: temporal_analysis.csv")
else:
    print("\n‚ö†Ô∏è  account_open_year –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö")

# ============================================================================
# 9. FEATURE IMPORTANCE (–±–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞)
# ============================================================================

print("\n" + "="*100)
print("[9/10] –û–¶–ï–ù–ö–ê –í–ê–ñ–ù–û–°–¢–ò –ü–†–ò–ó–ù–ê–ö–û–í")
print("="*100)

# –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –∫–∞–∫ –ø—Ä–æ–∫—Å–∏ –¥–ª—è –≤–∞–∂–Ω–æ—Å—Ç–∏
feature_importance = target_corr.copy()
feature_importance['importance_score'] = feature_importance['abs_correlation']

# –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–∏–ø–µ –ø—Ä–∏–∑–Ω–∞–∫–∞
feature_importance['is_numeric'] = feature_importance['feature'].apply(
    lambda x: x in numeric_features
)

print("\nüìä –¢–û–ü-30 –ù–ê–ò–ë–û–õ–ï–ï –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í (–ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏):")
print("="*100)
for i, (idx, row) in enumerate(feature_importance.head(30).iterrows(), 1):
    feat_type = "NUM" if row['is_numeric'] else "CAT"
    print(f"{i:2d}. [{feat_type}] {row['feature']:50s}: {row['correlation_with_default']:+.4f}")

# ============================================================================
# 10. –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò
# ============================================================================

print("\n" + "="*100)
print("[10/10] –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
print("="*100)

# –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –Ω–∞—Ö–æ–¥–∫–∏
findings = {
    'dataset_size': {
        'train': len(df_train),
        'test': len(X_test),
        'features': X_train.shape[1]
    },
    'class_balance': {
        'default_rate_train': default_rate_train,
        'default_rate_test': default_rate_test,
        'imbalance_ratio': imbalance_ratio
    },
    'feature_types': {
        'numeric': len(numeric_features),
        'categorical': len(categorical_features)
    },
    'data_quality': {
        'features_with_missing': len(features_with_missing),
        'high_skew_features': len(high_skew),
        'low_variance_features': len(low_variance),
        'high_zeros_features': len(high_zeros)
    },
    'potential_improvements': {
        'interaction_candidates': len(interaction_candidates) if len(interaction_candidates) > 0 else 0,
        'high_separation_features': len(sep_df[sep_df['ks_statistic'] > 0.3])
    }
}

import json
with open('/home/dr/cbu/dataset_analysis_summary.json', 'w') as f:
    json.dump(findings, f, indent=2)

print("\nüìä –ö–õ–Æ–ß–ï–í–´–ï –ù–ê–•–û–î–ö–ò:")
print("="*100)
print(f"\n‚úÖ –†–ê–ó–ú–ï–† –î–ê–ù–ù–´–•:")
print(f"   ‚Ä¢ Train: {findings['dataset_size']['train']:,}, Test: {findings['dataset_size']['test']:,}")
print(f"   ‚Ä¢ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {findings['dataset_size']['features']}")

print(f"\n‚öñÔ∏è  –ë–ê–õ–ê–ù–° –ö–õ–ê–°–°–û–í:")
print(f"   ‚Ä¢ Default rate: {findings['class_balance']['default_rate_train']:.2%}")
print(f"   ‚Ä¢ Imbalance: 1:{findings['class_balance']['imbalance_ratio']:.1f}")

print(f"\nüîç –ö–ê–ß–ï–°–¢–í–û –î–ê–ù–ù–´–•:")
print(f"   ‚Ä¢ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏: {findings['data_quality']['features_with_missing']}")
print(f"   ‚Ä¢ –° –≤—ã—Å–æ–∫–æ–π –∞—Å–∏–º–º–µ—Ç—Ä–∏–µ–π: {findings['data_quality']['high_skew_features']}")
print(f"   ‚Ä¢ –° –Ω–∏–∑–∫–æ–π –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å—é: {findings['data_quality']['low_variance_features']}")
print(f"   ‚Ä¢ –° >50% –Ω—É–ª–µ–π: {findings['data_quality']['high_zeros_features']}")

print(f"\nüí° –í–û–ó–ú–û–ñ–ù–û–°–¢–ò –î–õ–Ø –£–õ–£–ß–®–ï–ù–ò–Ø:")
print(f"   ‚Ä¢ –ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –∏–Ω—Ç–µ—Ä–∞–∫—Ü–∏–∏: {findings['potential_improvements']['interaction_candidates']}")
print(f"   ‚Ä¢ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —Ö–æ—Ä–æ—à–µ–π —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç—å—é: {findings['potential_improvements']['high_separation_features']}")

print("\n" + "="*100)
print("üìÅ –°–û–ó–î–ê–ù–ù–´–ï –§–ê–ô–õ–´:")
print("="*100)
print("   1. numeric_features_statistics.csv       - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
print("   2. categorical_features_statistics.csv   - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
print("   3. class_separation_analysis.csv         - –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤")
print("   4. interaction_candidates.csv            - –ö–∞–Ω–¥–∏–¥–∞—Ç—ã –Ω–∞ –∏–Ω—Ç–µ—Ä–∞–∫—Ü–∏–∏")
print("   5. temporal_analysis.csv                 - –í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑")
print("   6. dataset_analysis_summary.json         - –ò—Ç–æ–≥–æ–≤—ã–π JSON –æ—Ç—á–µ—Ç")

print("\n" + "="*100)
print("‚úÖ –ì–õ–£–ë–û–ö–ò–ô –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")
print("="*100)

print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø –°–õ–ï–î–£–Æ–©–ò–• –®–ê–ì–û–í:")
print("   1. –ó–∞–ø—É—Å—Ç–∏—Ç—å ml-data-preparation-specialist –¥–ª—è feature engineering")
print("   2. –ó–∞–ø—É—Å—Ç–∏—Ç—å research-specialist –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤")
print("   3. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ü–∏–∏ –∏–∑ interaction_candidates.csv")
print("   4. –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å binning –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –≤—ã—Å–æ–∫–æ–π –∞—Å–∏–º–º–µ—Ç—Ä–∏–µ–π")
print("   5. –ü—Ä–∏–º–µ–Ω–∏—Ç—å advanced sampling techniques –¥–ª—è class imbalance")
